{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c8072ff-7a8c-4065-ad36-3eb354d6c81d",
   "metadata": {},
   "source": [
    "# **üéØAI-Powered Early Disease Prediction using Multi-Source Health Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff5cbdf-0782-4d33-82dc-e4b6ec462551",
   "metadata": {},
   "source": [
    "![Image Alt Text](project_image_.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef36ac4-b18f-475a-9213-6d6c30f8fbaa",
   "metadata": {},
   "source": [
    "# üìë Table of Contents\n",
    "\n",
    "1. üìå Introduction\n",
    "2. üéØ Goal of the Project\n",
    "3. üìä Data Story\n",
    "4. üßπ Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f8b760-a210-4a3c-bd1b-273542bf41f8",
   "metadata": {},
   "source": [
    "## üìå Introduction\n",
    "\n",
    "In today‚Äôs fast-paced world, chronic illnesses such as **Diabetes**, **Heart Disease**, and **Parkinson‚Äôs Disease** are becoming more widespread, often going undiagnosed until advanced stages. Early detection is key to improving outcomes and lowering long-term healthcare costs.\n",
    "\n",
    "This project, titled **\"AI-Powered Early Disease Prediction using Multi-Source Health Data,\"** aims to develop a machine learning model that accurately predicts the likelihood of developing these diseases. By analyzing **clinical health records**, **lifestyle indicators**, and **medical measurements**, the model can help identify risk factors and issue early warnings.\n",
    "\n",
    "We utilize three real-world datasets:  \n",
    "- üß† **Parkinson‚Äôs Disease Dataset**  \n",
    "- ‚ù§Ô∏è **Heart Disease Dataset**  \n",
    "- üíâ **Diabetes Dataset**\n",
    "\n",
    "The power of **AI and classification algorithms** enables our system to learn patterns and deliver reliable, data-driven insights. This project showcases how **integrating multiple health sources** and applying **robust preprocessing and modeling techniques** can support **preventive healthcare** and assist professionals in making faster, smarter decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097847d-3b20-4308-aefc-f4a4ff3bf295",
   "metadata": {},
   "source": [
    "## üéØ Goal of the Project\n",
    "\n",
    "The primary goal of this project is to develop a machine learning model that can **predict early signs of disease** using health-related data. By analyzing multiple health indicators such as glucose levels, blood pressure, BMI, and age, the project aims to:\n",
    "\n",
    "- Detect patterns and risk factors associated with disease onset  \n",
    "- Assist in early diagnosis and preventive healthcare  \n",
    "- Empower healthcare professionals with data-driven insights  \n",
    "- Improve patient outcomes through timely predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c8bf9d-f87c-4146-8dec-7de9e51bd654",
   "metadata": {},
   "source": [
    "## üìà Data Story \n",
    "## üìä About the Dataset\n",
    "\n",
    "The datasets used in this project are sourced from the **UC Irvine Machine Learning Repository**, a trusted platform for machine learning research and experimentation.\n",
    "\n",
    "## üîó Source of Datasets\n",
    "\n",
    "The datasets used in this project are publicly available from the **UCI Machine Learning Repository**:\n",
    "\n",
    "- **Diabetes Dataset:**  \n",
    "  Source Link:https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database\n",
    "\n",
    "- **Heart Disease Dataset:**  \n",
    "  Source Link:https://archive.ics.uci.edu/dataset/45/heart+disease\n",
    "\n",
    "- **Parkinson's Disease Dataset:**  \n",
    "  Source Link:https://archive.ics.uci.edu/ml/datasets/parkinsons\n",
    "These datasets provide medical and clinical data that are essential for training models to predict early signs of disease.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üìù **Dataset Description**\n",
    "\n",
    "The project involves three distinct datasets, each tailored to predict a specific health condition using numerical and categorical clinical features. The goal is to **detect early signs of disease** using predictive modeling.\n",
    "\n",
    "- **Diabetes Dataset:** 768 entries √ó 9 columns\n",
    "- **Heart Disease Dataset:** 303 entries √ó 14 columns\n",
    "- **Parkinson's Disease Dataset:** 195 entries √ó 24 columns\n",
    "\n",
    "Each dataset includes both input features (health metrics) and a target column indicating presence or absence of disease.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç **Features/Columns Overview**\n",
    "### ü©∫ 1. Diabetes Dataset\n",
    "- **Samples:** 768  \n",
    "- **Features:** Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age  \n",
    "- **Target:** Outcome (0 = No Diabetes, 1 = Diabetes)  \n",
    "- **Goal:** Identify individuals at risk of diabetes based on clinical and personal health parameters.\n",
    "- ### ‚ù§Ô∏è 2. Heart Disease Dataset\n",
    "- **Samples:** 303  \n",
    "- **Features:** Age, Sex, Chest Pain Type, Resting Blood Pressure, Cholesterol, Fasting Blood Sugar, ECG, Max Heart Rate, etc.  \n",
    "- **Target:** Target (1 = Heart Disease, 0 = No Heart Disease)  \n",
    "- **Goal:** Predict presence of heart disease using key cardiovascular indicators.\n",
    "\n",
    "### üß† 3. Parkinson's Disease Dataset\n",
    "- **Samples:** 195  \n",
    "- **Features:** 22 voice measurements including frequency, jitter, shimmer, and other biomedical voice markers  \n",
    "- **Target:** Status (1 = Parkinson's Disease, 0 = Healthy)  \n",
    "- **Goal:** Use vocal biomarkers to detect the presence of Parkinson's Disease at early stages.\n",
    "---\n",
    "\n",
    "### üß∞ **Tools Used**\n",
    "- **Python**\n",
    "- **Pandas** ‚Äì Data handling\n",
    "- **Matplotlib & Seaborn** ‚Äì Data visualization\n",
    "- **Scikit-learn** ‚Äì Machine learning and model evaluation\n",
    "- **Jupyter Notebook** ‚Äì Interactive development\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **Data Story Summary**\n",
    "\n",
    "This project explores how diverse health features can be used to **predict diseases at early stages**. Each dataset presents a different perspective on human health ‚Äî from blood glucose levels to heart function and vocal signal patterns. By integrating them into one unified machine learning workflow, the goal is to **enhance early diagnosis and promote preventive care** through data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9934236d-f6b7-4914-90f1-1a199c7252a9",
   "metadata": {},
   "source": [
    "## üßπ Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a084390b-f7fd-4039-bcbc-5572b1a9bf94",
   "metadata": {},
   "source": [
    "**1. üìöImporting Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b929eb0f-66f7-47d9-8beb-39e5abbcfda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Basic Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# üìä Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ‚öôÔ∏è Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PowerTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# üéØ Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# üîÄ Data Splitting\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# ü§ñ Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# üìè Model Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    RocCurveDisplay\n",
    ")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# üíæ Model Saving\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb724d69-2a07-4b4c-b841-026469ced635",
   "metadata": {},
   "source": [
    "**2.üìÇ  Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7568a155-767b-4e7d-ae97-6739dd4172cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "diabetes = pd.read_csv(\"diabetes.csv\")\n",
    "heart = pd.read_csv(\"heart (1).csv\")\n",
    "parkinsons = pd.read_csv(\"parkinsons.csv\")\n",
    "parkinsons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745cc1c8-59c6-4c90-a713-c61d57c09be9",
   "metadata": {},
   "source": [
    "**3.Understand the data structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c9480-ff37-41e1-ba8a-b2e57ba8cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fe939-5c7b-4a50-b998-27bcc8503940",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04f733-29b7-48df-8a52-e3db961effa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinsons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99fbe78-4e7e-41f5-92ac-e910e227ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the dataset:\")\n",
    "diabetes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48bf32e-5a19-4b3e-bae8-1d21262e296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the dataset:\")\n",
    "heart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3d219-059a-45ae-bb3f-753360208bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the dataset:\")\n",
    "parkinsons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad01933-998c-4c4c-9afe-a9ae430ddea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Info:\")\n",
    "diabetes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dfa4bd-783a-48a1-82f8-ad89a9e4e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Info:\")\n",
    "heart.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4677bd1-fb89-4ab0-8423-fefa9675e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Info:\")\n",
    "parkinsons.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f78e09-903c-4df3-b557-ede99805f8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical Summary:\")\n",
    "diabetes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6a261-e566-4bba-9cf9-da2f3232a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical Summary:\")\n",
    "heart.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2c9fd-a22b-4c34-8e87-98d4c7645b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical Summary:\")\n",
    "parkinsons.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c534557e-0bcc-48f7-9024-730d7ac3cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac62869-65ad-4133-836d-b7f8afeb3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b9a80f-e38b-40de-a6e3-f992437838eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinsons.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ea0dc-57ed-43ca-91dd-d7bc75c173a0",
   "metadata": {},
   "source": [
    "## üî∂ 1. Diabetes Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8be81b-3ebb-4bfd-860c-c61a1af10e7c",
   "metadata": {},
   "source": [
    "**5.Handle Missing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd395052-44d5-40d1-9dea-d7c9967ced8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "missing_values = diabetes.isnull().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f8089a-40fe-4569-be08-e7c01a4949ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîö Final Check\n",
    "print(diabetes.isnull().sum())  # conform all should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d6759-3340-40b3-ac96-59df973d6e9e",
   "metadata": {},
   "source": [
    "**6.Handle Duplicates**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d3e63-e899-49ee-83b9-25bcf9865b37",
   "metadata": {},
   "source": [
    "‚úÖ Check & Remove Duplicate Rows in Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57095a21-2ac3-4956-b027-ffaa705a2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(\"Duplicate Rows:\", diabetes.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845ad4f-5274-4bc3-b8d1-931be581c694",
   "metadata": {},
   "source": [
    "**7.Handle outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b8b700-676f-489f-a55b-de53e6cd1791",
   "metadata": {},
   "source": [
    "‚úÖ Step: Check Skewness in the  Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32231931-34eb-44bc-a05a-e0da39132e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = diabetes.columns\n",
    "\n",
    "for i in columns:\n",
    "    print(f\"Skewness of {i}: {diabetes[i].skew()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa074642-2b10-4a25-8970-d8212e84bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes.skew().sort_values(ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6079fe81-a1bf-4874-a52f-d82b689b5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot to detect potential outliers in  data\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(diabetes)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplot of  Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff56e161-de11-4447-82a0-2c09fe880d68",
   "metadata": {},
   "source": [
    "### üì¶ Outlier Removal Based on Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f298b59-7260-4f77-9980-afdfb4bf74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå IQR method for removing outliers from selected columns\n",
    "def remove_outliers(diabetes, columns):\n",
    "    data_filtered = diabetes.copy()\n",
    "\n",
    "    for column in columns:\n",
    "        Q1 = diabetes[column].quantile(0.25)\n",
    "        Q3 = diabetes[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        data_filtered = data_filtered[\n",
    "            (data_filtered[column] >= lower_bound) & (data_filtered[column] <= upper_bound)\n",
    "        ]\n",
    "\n",
    "    return data_filtered\n",
    "\n",
    "# ‚úÖ Example usage (replace 'data' with your actual DataFrame)\n",
    "columns_to_clean = ['Pregnancies', 'BloodPressure','Insulin','DiabetesPedigreeFunction', 'Age']\n",
    "cleaned_data = remove_outliers(diabetes, columns_to_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7fe60b-f9c5-4759-bd6e-53592be65c61",
   "metadata": {},
   "source": [
    "Removing the outliers from 'Pregnancies', 'BloodPressure','Insulin','DiabetesPedigreeFunction', 'Age' by using remove_ outliers function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c186e3d-6362-4ada-92ac-d971d3032f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize boxplot after removing outliers\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(cleaned_data)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplot of  Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1711b267-7183-4d00-a199-bc9ca4349953",
   "metadata": {},
   "source": [
    "After analyzing the skewness of each column in the dataset, we identified that five features ‚Äî **Insulin**, **DiabetesPedigreeFunction**, **Age**, **BloodPressure**, and **Pregnancies** ‚Äî exhibited high skewness and contained significant outliers. To improve data quality and reduce noise, we applied the IQR (Interquartile Range) method to these columns to remove the extreme values.\n",
    "\n",
    "The boxplot below, generated after outlier removal, clearly shows that the distribution of these features has become more balanced, with fewer extreme data points. This step is essential for ensuring that the machine learning model is trained on clean, representative data, improving its performance and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd78e61-d293-4fc0-a3a4-cbad7c49b76b",
   "metadata": {},
   "source": [
    "### üîç Skewness Check After Outlier Removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5327e-97af-4ca5-a515-8092ca1d37ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Skewness After Outlier Removal:\")\n",
    "cleaned_data.skew().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcb3d68-1545-4047-88ad-cfa204cb959a",
   "metadata": {},
   "source": [
    "After IQR method there is still outliers in Pregnancies, BloodPressure,Insulin, DiabetesPedigreeFunction and Age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958200da-adcd-461e-8157-c8b6bfe7030d",
   "metadata": {},
   "source": [
    "  8.Handle Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2742ec1-97c5-4c56-952a-8d86d86bc017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying log transformation to skewed features\n",
    "# Applying log transformation to any feature with skewness > 1 right skewed\n",
    "# apply square root transformation when skewness is between 0.5 and 1\n",
    "new_data2= cleaned_data.copy() #creating a copy before skewness corrections\n",
    "for col in cleaned_data.columns:\n",
    "    if cleaned_data[col].skew() > 1:\n",
    "        cleaned_data[col] = np.log1p(cleaned_data[col])\n",
    "\n",
    "print(\"\\nSkewness after log transformation:\")\n",
    "print(cleaned_data.skew().sort_values(ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63b0f86-1389-402d-8d9a-19858fd2f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289942f8-1d5c-4a68-b328-bb4eabdeaf02",
   "metadata": {},
   "source": [
    "### üîÉ Handling Skewness in the Diabetes Dataset\n",
    "\n",
    "After outlier removal, we identified some features with remaining high skewness such as BloodPressure,Insulin, DiabetesPedigreeFunction and Age\n",
    "\n",
    "To normalize these distributions, we applied the **log transformation (log1p)**. This compresses large values and spreads out small ones, reducing the skew.\n",
    "\n",
    "Handling skewness improves model performance by ensuring that features follow a more normal distribution, which benefits algorithms like Logistic Regression, SVM, and Linear models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a32fa-3f01-480d-b65d-a427613689a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate categorical and numerical columns\n",
    "categorical_cols =cleaned_data.select_dtypes(include='int64').columns.tolist()\n",
    "numerical_cols = cleaned_data.select_dtypes(include='float64').columns.tolist()\n",
    "\n",
    "# Manually adjust if needed\n",
    "print(\"Categorical Columns:\", categorical_cols)\n",
    "print(\"Numerical Columns:\", numerical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503add4-eb36-493b-9c23-c0f7c81c1f96",
   "metadata": {},
   "source": [
    "## üß™ Exploratory Data Analysis (EDA)\n",
    "This section explores how each health-related feature in the dataset behaves individually, in pairs, and across multiple variables. These visuals help us understand patterns and relationships that are critical for building accurate AI disease prediction models.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d8a6f5-e0df-43ee-87cf-70f9c4827585",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ üìä Histogram(Univariate Analysis) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e1768-b2d7-47de-8ed7-6a5735f6feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Outcome and calculate mean for each feature\n",
    "grouped_means = cleaned_data.groupby(\"Outcome\").mean().T\n",
    "\n",
    "# Plot grouped bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "grouped_means.plot(kind='bar', figsize=(12, 6), color=['#66c2a5', '#fc8d62'], width=0.75)\n",
    "plt.title(\"üìä Average Feature Values by Diabetes Outcome\", fontsize=14)\n",
    "plt.xlabel(\"Health Features\", fontsize=12)\n",
    "plt.ylabel(\"Average Value\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend([\"No Diabetes (0)\", \"Diabetes (1)\"], title=\"Outcome\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c663a8-3287-4ac0-9f8d-09a57840c357",
   "metadata": {},
   "source": [
    "### üìä Average Feature Values by Diabetes Outcome\n",
    "\n",
    "The above grouped bar chart visualizes the average values of various health features for two patient groups ‚Äî those **with diabetes (Outcome = 1)** and those **without diabetes (Outcome = 0)**.\n",
    "\n",
    "Each feature from the dataset is represented on the x-axis, and the average value of that feature is shown on the y-axis for both classes. This chart helps us easily compare how each health metric varies between diabetic and non-diabetic individuals.\n",
    "\n",
    "#### üßæ Key Observations:\n",
    "- **Glucose**: Diabetic patients have a significantly higher average glucose level than non-diabetic ones. This is one of the most distinguishing features.\n",
    "- **BMI** and **Age**: These are also higher on average for diabetic individuals.\n",
    "- **BloodPressure**, **SkinThickness**, and **DiabetesPedigreeFunction** show slight differences.\n",
    "- **Pregnancies**: Diabetic patients tend to have a higher number of pregnancies.\n",
    "- **Insulin**: Only a small visible difference, possibly due to normalization or missing values.\n",
    "\n",
    "#### ‚úÖ Conclusion:\n",
    "This visual is important because it shows which features have the most impact on the diagnosis of diabetes. These patterns can help in selecting relevant features for machine learning models and building accurate disease prediction systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b95ef-c5a1-46f0-a561-18b22a62714b",
   "metadata": {},
   "source": [
    "2Ô∏è‚É£  üîó Pair Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed83b4-e7a9-443d-a1d6-386ea8c984e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Pair Plot \n",
    "# Select only the key numeric columns (excluding 'Outcome' if needed)\n",
    "selected_columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'BMI', 'Age', 'Outcome']\n",
    "\n",
    "# Plot pair plot\n",
    "sns.pairplot(cleaned_data[selected_columns], hue='Outcome', palette='husl', diag_kind='kde')\n",
    "plt.suptitle(\"Pair Plot of Key Features by Diabetes Outcome\", y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe5dbaa-d28f-4bd6-88d3-e1cdb79b3fb8",
   "metadata": {},
   "source": [
    "### üîç Pair Plot of Key Features by Diabetes Outcome\n",
    "\n",
    "The pair plot above shows the relationships between selected health features and how they vary with the diabetes outcome. Each cell in the grid represents a scatter plot of one feature against another, helping identify correlations and class separation.\n",
    "\n",
    "#### üîë Observations:\n",
    "- **Glucose** and **BMI** show clear separation between diabetic and non-diabetic patients.\n",
    "- **Age** and **Pregnancies** also demonstrate upward trends for diabetic cases.\n",
    "- Diagonal plots show KDE (density) curves, giving insights into how each feature is distributed for each class (Outcome 0 vs Outcome 1).\n",
    "\n",
    "#### ‚úÖ Why This Visual Matters:\n",
    "This \"gara graph\" gives a simple and powerful overview of feature behavior, helping data scientists and healthcare professionals understand which health metrics are most useful for early disease prediction. It also supports effective feature selection for building AI models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2b1076-0e04-4de6-9351-a33366178559",
   "metadata": {},
   "source": [
    "3Ô∏è‚É£ ü•ß Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0d439-f167-4b8b-a14c-c6806cdeba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count values in Outcome column\n",
    "labels = ['No Diabetes (0)', 'Diabetes (1)']\n",
    "sizes = cleaned_data['Outcome'].value_counts().sort_index()\n",
    "colors = ['#66c2a5', '#fc8d62']\n",
    "\n",
    "# Create pie chart\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, shadow=True, explode=(0, 0.05))\n",
    "plt.title(\"üß¨ Diabetes Outcome Distribution\", fontsize=14)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures a perfect circle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dbc599-c23b-4286-bd29-492c0d5c61b7",
   "metadata": {},
   "source": [
    "### ü•ß Pie Chart: Diabetes Outcome Distribution\n",
    "\n",
    "This pie chart shows the distribution of diabetes outcomes in the dataset. It compares the proportion of patients who were diagnosed with diabetes (`Outcome = 1`) versus those who were not (`Outcome = 0`).\n",
    "\n",
    "#### üìä Interpretation:\n",
    "- The chart clearly shows whether the dataset is **balanced or imbalanced**.\n",
    "- A higher percentage of either class helps understand how models might behave (e.g., biased toward the majority class).\n",
    "\n",
    "#### ‚úÖ Why This Visual Matters:\n",
    "This \"gara graph\" provides a simple, attractive overview of the dataset's target label. It‚Äôs helpful for doctors, data scientists, or AI systems to see the overall ratio of positive to negative cases before training a prediction model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c647426-edec-43e1-b092-b9a92f2a032a",
   "metadata": {},
   "source": [
    "4Ô∏è‚É£  üß™ Bar Chart ‚Äì Average Glucose by Diabetes Outcome (Bivariate Analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6064241-3873-41c7-b430-431287faabf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average glucose by Outcome\n",
    "glucose_avg = cleaned_data.groupby('Outcome')['Glucose'].mean().reset_index()\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x='Outcome', y='Glucose', data=glucose_avg, palette='pastel')\n",
    "plt.title(\"üß™ Average Glucose by Diabetes Outcome\", fontsize=14)\n",
    "plt.xlabel(\"Outcome (0 = No Diabetes, 1 = Diabetes)\")\n",
    "plt.ylabel(\"Average Glucose Level\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed095de-e190-4164-9c85-7565a9504773",
   "metadata": {},
   "source": [
    "### üß™ Bar Chart ‚Äì Average Glucose by Diabetes Outcome (Bivariate Analysis)\n",
    "\n",
    "This bar chart displays the **average glucose levels** of patients categorized by their diabetes outcome:\n",
    "\n",
    "- **Outcome = 0** represents patients without diabetes.\n",
    "- **Outcome = 1** represents patients with diabetes.\n",
    "\n",
    "#### üîç Observations:\n",
    "- Patients diagnosed with diabetes have a **noticeably higher average glucose level** than those without.\n",
    "- This suggests that **glucose is a key feature** in predicting the presence of diabetes.\n",
    "\n",
    "#### ‚úÖ Why This Visual Matters:\n",
    "This bivariate analysis highlights a strong relationship between glucose levels and diabetes. It supports the idea that elevated glucose is a major indicator for early disease detection, making it an essential feature in any AI-powered predictive model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93275c45-bc6e-4487-98a8-8d7f1b1d6d83",
   "metadata": {},
   "source": [
    "5Ô∏è‚É£üß™ Histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3e579-32c2-4dae-9d6d-5568b21abe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(cleaned_data['Glucose'], kde=True, color='#66c2a5', bins=30)\n",
    "plt.title(\"üß™ Distribution of Glucose Levels\", fontsize=14)\n",
    "plt.xlabel(\"Glucose\")\n",
    "plt.ylabel(\"Number of Patients\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b6252f-8ceb-4c90-8429-e276e0d232ae",
   "metadata": {},
   "source": [
    "### üß™ Histogram: Glucose Level Distribution (Univariate Analysis)\n",
    "\n",
    "This histogram shows the distribution of **glucose levels** among patients in the dataset. It is a classic example of univariate analysis, where we examine the behavior of a single variable.\n",
    "\n",
    "#### üîç Observations:\n",
    "- Most patients have glucose values between **80 and 140**.\n",
    "- A noticeable right skew indicates that some patients have very high glucose levels.\n",
    "- Glucose is a key indicator in predicting diabetes, and understanding its spread is vital.\n",
    "\n",
    "#### ‚úÖ Why This Visual Matters:\n",
    "This gara graph highlights how glucose varies across the population and helps identify thresholds or risk ranges. It serves as a simple, standalone insight into one of the most important features in early disease prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1dd03-a224-4dcd-879a-7090c9b51348",
   "metadata": {},
   "source": [
    "6Ô∏è‚É£ üì¶ Boxplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1bdd2-0f9c-4445-90cf-4c852c775225",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "sns.boxplot(x='Outcome', y='BMI', data=cleaned_data, palette='Set2')\n",
    "plt.title(\"üì¶ BMI by Diabetes Outcome\", fontsize=14)\n",
    "plt.xlabel(\"Outcome (0 = No Diabetes, 1 = Diabetes)\")\n",
    "plt.ylabel(\"BMI\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977951a-9a39-451f-bf65-48300d3a9a06",
   "metadata": {},
   "source": [
    "### üì¶ Boxplot: BMI by Diabetes Outcome (Bivariate Analysis)\n",
    "\n",
    "This bivariate visual compares the **BMI distribution** between patients with and without diabetes. Each box shows the spread of BMI values for each outcome class.\n",
    "\n",
    "#### üîç Observations:\n",
    "- Diabetic patients (Outcome = 1) tend to have higher BMI values.\n",
    "- The median BMI is higher in the diabetic group.\n",
    "- There are outliers in both groups, but more extreme in the diabetic class.\n",
    "\n",
    "#### ‚úÖ Why This Visual Matters:\n",
    "This gara graph helps show the **relationship between BMI and diabetes**, confirming that higher BMI is often associated with a higher risk of developing diabetes. It gives insight into how two variables interact ‚Äî perfect for feature relevance in disease prediction models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f15c3e-0516-472f-a1cb-b5f8a060bff4",
   "metadata": {},
   "source": [
    "7Ô∏è‚É£üî• Correlation Heatmap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a562bafe-3358-4e64-bd8f-d8edd1e5f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "corr_matrix = cleaned_data.corr()\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"üîç Correlation Heatmap of Features\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe0d4d7-a530-40fa-8992-bd5cf2b1a57b",
   "metadata": {},
   "source": [
    "### üîç Correlation Heatmap of Features (Multivariate Analysis)\n",
    "\n",
    "This heatmap shows the **pairwise correlations** between all numeric features in the dataset. It helps identify how strongly one feature is related to another, including how much each one contributes to the diabetes prediction (`Outcome`).\n",
    "\n",
    "#### üîç Observations:\n",
    "- **Glucose** has the highest positive correlation with `Outcome`, indicating it's a strong predictor of diabetes.\n",
    "- **BMI**, **Age**, and **Pregnancies** also show moderate positive correlations.\n",
    "- Some features like **SkinThickness** and **BloodPressure** are weakly correlated with the outcome.\n",
    "\n",
    "#### ‚úÖ Why This Visual Matters:\n",
    "This gara graph helps in **feature selection**. Highly correlated features may contain similar information, and low-correlated ones may not be helpful. The heatmap is a go-to visual in AI and ML pipelines to understand data relationships before modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45126c0-93a0-4762-97eb-181a77e567d9",
   "metadata": {},
   "source": [
    "### üß† Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef3b96-1748-40db-9d08-c0fd284c74bf",
   "metadata": {},
   "source": [
    "### üî° Encoding\n",
    "\n",
    "In this dataset, all columns are already in numerical format and represent measurable health-related features such as:\n",
    "\n",
    "- **Pregnancies**\n",
    "- **Glucose**\n",
    "- **BloodPressure**\n",
    "- **SkinThickness**\n",
    "- **Insulin**\n",
    "- **BMI**\n",
    "- **DiabetesPedigreeFunction**\n",
    "- **Age**\n",
    "- **Outcome**\n",
    "\n",
    "Since there are no categorical (text-based) columns like gender, region, or status, **encoding is not required** at this stage.\n",
    "\n",
    "Had there been categorical variables, we would have used **Label Encoding** or **One-Hot Encoding** to convert them into numerical values suitable for machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872aa422-d3b5-41d3-ad4f-5019a8cf1df0",
   "metadata": {},
   "source": [
    "### ‚úÖ Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bccf0f4-923a-49aa-84f3-8d927bfd6c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 6))\n",
    "correlation = cleaned_data.corr()\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(correlation, annot=True, cmap='YlGnBu')\n",
    "plt.title(\"üîç Correlation Heatmap for Feature Selection\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c9f8dd-ff09-4da3-b878-e461f5651fea",
   "metadata": {},
   "source": [
    "### üéØ Target Variable\n",
    "\n",
    "In this project, the target variable is:\n",
    "\n",
    "**`Outcome`**\n",
    "\n",
    "This column represents whether a patient is diabetic:\n",
    "- `0` ‚Üí The patient does **not** have diabetes\n",
    "- `1` ‚Üí The patient **has** diabetes\n",
    "\n",
    "All other columns (like Glucose, BMI, Age, etc.) are input features used to train the AI model to predict this outcome. Accurately predicting this target variable is the goal of the machine learning model in this early disease prediction project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d7a56f-1aeb-4824-b3cc-7da9a73b4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation with target variables\n",
    "print( cleaned_data.corr()['Outcome'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ebcee9-e804-4761-92f2-20b3be5d1c0f",
   "metadata": {},
   "source": [
    "### ‚úÖ Final Feature Selection Based on Correlation\n",
    "\n",
    "Using Pearson correlation analysis with the target variable (`Outcome`), we identified the strength of relationships between each feature and diabetes diagnosis.\n",
    "\n",
    "#### üìä Top Features Selected:\n",
    "- `Glucose` (correlation = 0.459)\n",
    "- `Age` (correlation = 0.301)\n",
    "- `BMI` (correlation = 0.261)\n",
    "- `Pregnancies` (correlation = 0.230)\n",
    "\n",
    "These features have the highest positive correlation with diabetes and were selected for training the machine learning model.\n",
    "\n",
    "Other features like **Insulin**, **SkinThickness**, and **DiabetesPedigreeFunction** showed low or no significant correlation and were not included in the final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64189d-2ff8-466b-b3a3-51c1a34eae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features (X) and target (y)\n",
    "X = cleaned_data.drop('Outcome', axis=1)\n",
    "y = cleaned_data['Outcome']\n",
    "\n",
    "# Apply SelectKBest with f_classif scoring function\n",
    "select_k = SelectKBest(score_func=f_classif, k=4)  # Selecting top 1 features\n",
    "X_selected = select_k.fit_transform(X, y)\n",
    "print(\"Selected Features:\", X.columns[select_k.get_support()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac45c4-90e7-4d60-9fdd-29c43b113960",
   "metadata": {},
   "source": [
    "### ‚úÖ Feature Selection using SelectKBest (k = 4)\n",
    "\n",
    "To improve model accuracy and reduce noise, we applied **feature selection** using the `SelectKBest` method with the **ANOVA F-test** (`f_classif`) scoring function. This helps us select the top features that are most statistically significant in predicting diabetes (`Outcome`).\n",
    "\n",
    "We selected the top **4 features** (`k=4`) based on their F-scores.\n",
    "\n",
    "#### üèÜ Selected Features:\n",
    "- `Glucose`\n",
    "- `Age`\n",
    "- `BMI`\n",
    "- `Pregnancies`\n",
    "\n",
    "These features showed the highest statistical influence on the outcome and will be used to train the AI-powered disease prediction model.\n",
    "\n",
    "Feature selection helps in:\n",
    "- Reducing overfitting\n",
    "- Improving model accuracy\n",
    "- Enhancing model performance and training speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d061d-36c4-4388-bae3-4f7b119b6be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_scores = select_k.scores_[select_k.get_support()] # to find scores of all features\n",
    "print(\"Feature Scores based on select_k:\", selected_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b24c5-32ef-45b4-b034-f7e389160fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on k scores we can choose number of features required\n",
    "select_k = SelectKBest(score_func=f_classif, k=2)  # Selecting top 2 features (depends on user choice)\n",
    "X_selected = select_k.fit_transform(X, y)\n",
    "\n",
    "print(\"Selected Features:\", X.columns[select_k.get_support()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151d3a7e-d1d0-4dd2-97ec-641508237597",
   "metadata": {},
   "source": [
    "**Split Data into Training and Testing Sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a2a38-d7af-4a8e-95d8-6592e2628c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define X and y again (if not already)\n",
    "X = cleaned_data.drop('Outcome', axis=1)\n",
    "y = cleaned_data['Outcome']\n",
    "\n",
    "# Split the data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Testing set size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98382383-44fe-41f5-862d-42154af1c075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a3a32-bd57-457b-a10b-9665080c7c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6488261-1f99-4bc0-b91c-178548e97212",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70231568-6a62-48be-b00c-759e1bd1eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52be2bdd-7021-475f-875f-52cb47206b25",
   "metadata": {},
   "source": [
    "### ‚öñÔ∏è Applying SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05229a6a-c369-449b-9bf6-6f2039dfcf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before SMOTE\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=y_train, palette=\"Set2\")\n",
    "plt.title(\"üìâ Class Distribution Before SMOTE\")\n",
    "plt.xlabel(\"Outcome\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0885bd-cb6c-4a8c-b717-2d439d7c470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying SMOTE\n",
    "print(\"Original Class Distribution:\", y_train.value_counts())\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "print(\"Resampled Class Distribution:\", pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891353ec-eeb7-4d67-9b1f-6908342934d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ‚úÖ After SMOTE\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=y_train, palette=\"Set1\")\n",
    "plt.title(\"üìà Class Distribution After SMOTE\")\n",
    "plt.xlabel(\"Outcome\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a610b24b-de0f-467a-9d33-6b8225d927dc",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Handling Class Imbalance with SMOTE\n",
    "\n",
    "In the diabetes dataset, the target variable (`Outcome`) was **imbalanced** ‚Äî meaning there were significantly more non-diabetic cases (`0`) than diabetic cases (`1`).  \n",
    "This imbalance can cause machine learning models to become **biased**, predicting the majority class more often.  \n",
    "\n",
    "To solve this, we applied **SMOTE (Synthetic Minority Oversampling Technique)**:  \n",
    "- SMOTE generates synthetic samples of the minority class (`1` - diabetic).  \n",
    "- This balances the dataset and helps the model learn equally from both classes.  \n",
    "\n",
    "The count plots below clearly show the **class distribution before and after SMOTE**:\n",
    "- **Before SMOTE** ‚Üí Class `0` dominates.  \n",
    "- **After SMOTE** ‚Üí Both classes are balanced.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d3f3dc-70f9-4dc9-8067-9875b6f7e6ed",
   "metadata": {},
   "source": [
    "### ‚öñÔ∏è Feature Scaling ‚Äì Diabetes Dataset\n",
    "\n",
    "Feature scaling is an essential step before applying many machine learning algorithms, especially those that are **distance-based or gradient-based**, such as:\n",
    "\n",
    "- Logistic Regression\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Support Vector Machines (SVM)\n",
    "- Gradient Boosting\n",
    "\n",
    "In the diabetes dataset, features like `Glucose`, `Insulin`, `Age`, and `BMI` have different scales. Without scaling, models may give more weight to features with higher magnitudes.\n",
    "\n",
    "We used **StandardScaler**, which transforms the data to have:\n",
    "- Mean = 0\n",
    "- Standard deviation = 1\n",
    "\n",
    "This ensures all features contribute equally to the model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87943d00-112b-4b8e-a662-218c5bd96fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "print(X_train_scaled )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92630231-fa9b-45bb-ab1b-30c278611c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df = pd.DataFrame(y_train) #converting to data frame from series\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(y_train_df)\n",
    "y_train_scaled = scaler.transform(y_train_df)\n",
    "print(y_train_scaled )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c77c3-8483-4ddb-8d6b-f2a3e74abb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled#scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb3056-281d-4294-98ed-4e927a9748a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d04d5-32f4-4a12-a62f-c3fa84656aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60913c8a-06e4-4546-8b58-0da80ad938b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe148484-290d-46b9-93d0-5ae94c33f3b1",
   "metadata": {},
   "source": [
    "### ü§ñ Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79488dcb-0188-409d-8e95-3c2e7de68e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Build ML Models for Diabetes Dataset\n",
    "# Dictionary of models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"Support Vector Machine\": SVC(probability=True, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Train & Evaluate\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results.append([name, acc, prec, rec, f1])\n",
    "    print(f\"üîπ {name}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d281b-79b4-4261-a3eb-29fc38d0a55c",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Building Machine Learning Models\n",
    "\n",
    "To predict the likelihood of diabetes, we implemented and evaluated **five classification models**:  \n",
    "- Logistic Regression  \n",
    "- Support Vector Machine (SVM)  \n",
    "- Decision Tree  \n",
    "- Random Forest  \n",
    "- Gradient Boosting  \n",
    "\n",
    "Each model was trained on the processed dataset, and performance was compared using **Accuracy, Precision, Recall, and F1 Score**.  \n",
    "This ensures a fair evaluation and helps identify the most effective algorithm for early diabetes prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76256d7-ec75-41df-ba4c-01af3d704673",
   "metadata": {},
   "source": [
    "## üèÜ Best Performing Model\n",
    "\n",
    "After experimenting with five machine learning models:\n",
    "\n",
    "- Logistic Regression  \n",
    "- Support Vector Machine (SVM)  \n",
    "- Decision Tree  \n",
    "- Random Forest  \n",
    "- Gradient Boosting  \n",
    "\n",
    "We evaluated them based on **Accuracy, Precision, Recall, and F1-Score**.  \n",
    "Among these, **Gradient Boosting** outperformed the others by providing the best balance between accuracy and recall, which is very important in medical diagnosis (to correctly identify patients at risk of Diabetes).  \n",
    "\n",
    "üìå Therefore, we selected **Gradient Boosting Classifier** as our final model for this project.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea61c285-1340-480c-89ce-7cb2f0119e5f",
   "metadata": {},
   "source": [
    "üß≠  Overfitting is checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d934a-155b-4041-8dbf-a7b1b23ccff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Training Accuracy:\", train_acc)\n",
    "print(\"Testing Accuracy:\", test_acc)\n",
    "\n",
    "# Check overfitting\n",
    "if train_acc - test_acc > 0.1:\n",
    "    print(\"‚ö†Ô∏è Model may be Overfitting!\")\n",
    "else:\n",
    "    print(\"‚úÖ Model is Generalizing Well\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27153b80-59c7-4f34-b69e-85bdc8545347",
   "metadata": {},
   "source": [
    "‚úÖ Overfitting Check on Diabetes Dataset (Logistic Regression)\n",
    "\n",
    "Training Accuracy: 0.763\n",
    "\n",
    "Testing Accuracy: 0.754\n",
    "\n",
    "The gap between training and testing accuracy is very small (‚âà 0.009).\n",
    "This indicates that the Logistic Regression model on the Diabetes dataset is generalizing well.\n",
    "\n",
    "‚úîÔ∏è The model captures the underlying patterns without overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c58b9-49af-4b1b-8ed0-c79d433a7ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# üîß Hyperparameter Tuning with GridSearchCV\n",
    "# Define model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Apply GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,\n",
    "                           scoring='accuracy',\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"‚úÖ Best Parameters:\", grid_search.best_params_)\n",
    "print(\"üéØ Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72bbe8-6b18-4fc3-817d-9e055d91cd79",
   "metadata": {},
   "source": [
    "### üîç Hyperparameter Tuning with GridSearchCV  \n",
    "\n",
    "To improve Random Forest performance, we applied **GridSearchCV**, which tests multiple hyperparameter combinations and selects the best one.  \n",
    "\n",
    "- **n_estimators:** Number of trees in the forest.  \n",
    "- **max_depth:** Maximum depth of each tree (None = unlimited).  \n",
    "- **min_samples_split:** Minimum samples required to split a node.  \n",
    "- **min_samples_leaf:** Minimum samples required at a leaf node.  \n",
    "\n",
    "‚úÖ **GridSearchCV** performs cross-validation to avoid overfitting and gives the best parameter set.  \n",
    "\n",
    "After finding the best parameters, we retrain the model and evaluate it on the test set.  \n",
    "\n",
    "This ensures that the chosen Random Forest model is optimized and generalizes well on unseen data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a52a00f-2c6e-48d3-a3d6-4914fa8fadd8",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f180dad9-4c0c-4132-8e53-8ea086490331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Pipeline with SMOTE + Scaling + Random Forest + GridSearchCV\n",
    "\n",
    "# Define pipeline\n",
    "pipeline = ImbPipeline(steps=[\n",
    "    ('scaler', StandardScaler()),     # Feature scaling\n",
    "    ('smote', SMOTE(random_state=42)),# Handle class imbalance\n",
    "    ('rf', RandomForestClassifier(random_state=42))  # Model\n",
    "])\n",
    "\n",
    "# Define parameter grid (only for the RandomForest part)\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [100, 200],\n",
    "    'rf__max_depth': [None, 10, 20],\n",
    "    'rf__min_samples_split': [2, 5],\n",
    "    'rf__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=pipeline,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,\n",
    "                           scoring='accuracy',\n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "# Fit on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best Parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on Test Data\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7580a3b8-761a-4a15-a67b-9a9d9000f717",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Pipeline with SMOTE, Scaling, and Random Forest\n",
    "\n",
    "Instead of applying preprocessing steps manually, we use a **Pipeline**.  \n",
    "This ensures that all transformations happen consistently during **training and testing**.  \n",
    "\n",
    "Steps in the pipeline:  \n",
    "1. **StandardScaler** ‚Üí Scales the features so models work efficiently.  \n",
    "2. **SMOTE** ‚Üí Balances the dataset by generating synthetic minority class samples.  \n",
    "3. **Random Forest Classifier** ‚Üí Trains the model using the chosen hyperparameters.  \n",
    "\n",
    "We apply **GridSearchCV** on this pipeline to optimize Random Forest hyperparameters.  \n",
    "This way, scaling, balancing, and training are handled automatically during cross-validation.  \n",
    "\n",
    "‚úÖ The final model is robust, avoids data leakage, and is easier to reproduce.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55836f0c-1a13-4731-8138-dba712533fdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, auc,\n",
    "    precision_recall_curve, ConfusionMatrixDisplay,\n",
    "    classification_report\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predictions + probabilities\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"üìÑ Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"üßÆ ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "# Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.3f})\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\"); plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision‚ÄìRecall Curve\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "plt.figure()\n",
    "plt.plot(rec, prec)\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision‚ÄìRecall Curve\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e650728-ca3a-49ec-bef0-9ee1308ec409",
   "metadata": {},
   "source": [
    "# üìä Model Evaluation Report\n",
    "\n",
    "After training and tuning the Random Forest Classifier, we evaluated the model using multiple performance metrics and visualizations. Below is the detailed interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. Classification Report\n",
    "\n",
    "The classification report provides **precision, recall, F1-score, and support** for each class.\n",
    "\n",
    "- **Precision** ‚Üí Out of all predicted positives, how many were correct.  \n",
    "- **Recall (Sensitivity)** ‚Üí Out of all actual positives, how many were detected.  \n",
    "- **F1-Score** ‚Üí Harmonic mean of precision & recall (balance between them).  \n",
    "- **Support** ‚Üí Number of actual occurrences of the class in test data.\n",
    "\n",
    "### üìë Results:\n",
    "- **Class 0 (No Disease)**  \n",
    "  - Precision = **0.86**  \n",
    "  - Recall = **0.83**  \n",
    "  - F1-Score = **0.85**  \n",
    "  ‚Üí Model is very good at predicting no disease.\n",
    "\n",
    "- **Class 1 (Disease Present)**  \n",
    "  - Precision = **0.66**  \n",
    "  - Recall = **0.71**  \n",
    "  - F1-Score = **0.68**  \n",
    "  ‚Üí Model is weaker here, but still catches most cases.\n",
    "\n",
    "- **Overall Accuracy = 79%**  \n",
    "- **Macro F1 = 0.76** (balanced performance across classes).  \n",
    "- **Weighted F1 = 0.79** (accounts for class imbalance).\n",
    "\n",
    "‚úÖ Interpretation: The model favors class 0 slightly, but still detects most patients with the disease.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. Confusion Matrix\n",
    "\n",
    "The confusion matrix gives exact counts of predictions:\n",
    "\n",
    "|                  | Predicted 0 | Predicted 1 |\n",
    "|------------------|-------------|-------------|\n",
    "| **Actual 0**     | 74 (TN)     | 15 (FP)     |\n",
    "| **Actual 1**     | 12 (FN)     | 29 (TP)     |\n",
    "\n",
    "- **True Negatives (TN = 74)** ‚Üí Correctly predicted ‚ÄúNo Disease‚Äù.  \n",
    "- **False Positives (FP = 15)** ‚Üí Predicted disease but actually healthy.  \n",
    "- **False Negatives (FN = 12)** ‚Üí Missed real patients ‚ùå (critical in healthcare).  \n",
    "- **True Positives (TP = 29)** ‚Üí Correctly identified disease cases.  \n",
    "\n",
    "‚ö†Ô∏è Concern: **12 false negatives** ‚Üí patients wrongly predicted as healthy. In medical tasks, minimizing FN is more important than maximizing accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3. ROC Curve & AUC Score\n",
    "\n",
    "- **ROC (Receiver Operating Characteristic) Curve** ‚Üí Plots True Positive Rate (Recall) vs. False Positive Rate.  \n",
    "- **AUC (Area Under Curve)** = **0.827**  \n",
    "  - 1.0 = Perfect Model  \n",
    "  - 0.5 = Random Guessing  \n",
    "  - 0.827 = Good discriminative ability\n",
    "\n",
    "‚úÖ Interpretation: The model distinguishes well between classes.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 4. Precision-Recall Curve\n",
    "\n",
    "- **Precision-Recall Curve** is useful for **imbalanced datasets**.  \n",
    "- Our curve starts with **high precision (1.0)** but decreases as recall increases.  \n",
    "- When recall is high (catching more patients), precision drops (more false alarms).  \n",
    "\n",
    "‚úÖ Interpretation: The model performs well at moderate thresholds, but struggles when forced to detect every patient.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Overall Conclusion\n",
    "\n",
    "- The model performs well (Accuracy = 79%, AUC = 0.83).  \n",
    "- **Problem:** False Negatives (12) ‚Üí real patients missed.  \n",
    "- **Solution:**  \n",
    "  1. Use **class weights** or **SMOTE oversampling**.  \n",
    "  2. Adjust probability **threshold** (e.g., from 0.5 ‚Üí 0.4) to reduce false negatives.  \n",
    "  3. Try other classifiers (XGBoost, Logistic Regression) for comparison.  \n",
    "  4. Apply **cross-validation** for stability.  \n",
    "\n",
    "In healthcare, **Recall is more important** than overall accuracy, because missing a patient can be dangerous.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65359d4f-3be9-453d-98e0-10e0f5da4886",
   "metadata": {},
   "source": [
    " Save the Trained Model (using joblib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb4ef64-344e-4e8f-8068-0856f287821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'disease_prediction_model.pkl')\n",
    "print(\"‚úÖ Model saved successfully!\")\n",
    "# Load model later\n",
    "loaded_model = joblib.load(\"disease_prediction_model.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443b691-43d7-4cba-ac89-1393b5c81027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())  # shows current working directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad20d6d-ed02-4c7d-95ee-4b17c692c49a",
   "metadata": {},
   "source": [
    "üß™ Load and Use the Saved Model Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de0122-2fc4-4330-93cc-9495610525ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example unseen data (Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age)\n",
    "unseen_patient = np.array([[2, 120, 70, 25, 80, 28.5, 0.45, 32]])\n",
    "\n",
    "# Scale and predict (pipeline already has scaler + model)\n",
    "prediction = loaded_model.predict(unseen_patient)\n",
    "proba = loaded_model.predict_proba(unseen_patient)\n",
    "\n",
    "print(\"Predicted Class:\", \"Diabetes\" if prediction[0] == 1 else \"No Diabetes\")\n",
    "print(\"Prediction Probability:\", proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b1781-5550-4c9d-8cd4-1683bbcd82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# üß™ Example unseen patients' data\n",
    "# Columns: [Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age]\n",
    "unseen_patients = np.array([\n",
    "    [2, 120, 70, 25, 80, 28.5, 0.45, 32],   # Patient 1\n",
    "    [5, 155, 80, 32, 120, 34.0, 0.65, 45],  # Patient 2\n",
    "    [0, 95, 65, 20, 70, 22.0, 0.30, 25],    # Patient 3\n",
    "    [3, 180, 90, 35, 150, 37.5, 0.85, 50],  # Patient 4\n",
    "    [1, 130, 72, 28, 85, 30.2, 0.55, 29]    # Patient 5\n",
    "])\n",
    "\n",
    "# ‚úÖ Convert to DataFrame (recommended for readability)\n",
    "columns = [\n",
    "    \"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\",\n",
    "    \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"\n",
    "]\n",
    "unseen_df = pd.DataFrame(unseen_patients, columns=columns)\n",
    "\n",
    "# üéØ Predictions using loaded model pipeline\n",
    "predictions = loaded_model.predict(unseen_df)\n",
    "probabilities = loaded_model.predict_proba(unseen_df)\n",
    "\n",
    "# üìä Combine results\n",
    "results = unseen_df.copy()\n",
    "results[\"Prediction\"] = [\"Diabetes\" if p == 1 else \"No Diabetes\" for p in predictions]\n",
    "results[\"Prob_No_Diabetes\"] = probabilities[:, 0]\n",
    "results[\"Prob_Diabetes\"] = probabilities[:, 1]\n",
    "\n",
    "# üñ®Ô∏è Show results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d388f934-b1eb-4fa4-95a7-5cc95fc17895",
   "metadata": {},
   "source": [
    "### üß™  Testing with Unseen Data\n",
    "\n",
    "Now that the model is saved, we must test it with **completely new patient records** that were never seen during training/testing.  \n",
    "\n",
    "- Input: A new patient's medical data.  \n",
    "- Output: Predicted **Diabetes / No Diabetes** and the **probability score**.  \n",
    "\n",
    "This step ensures that our model can be applied to **real-world cases** and is not just memorizing the training data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0dc14f-ac4a-4335-9a7b-88297e5013fd",
   "metadata": {},
   "source": [
    "üîú Next Step: Model Evaluation Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80b7e1a-2d83-4d11-a09a-0b3374d497ec",
   "metadata": {},
   "source": [
    "## üìù Diabetes Dataset ‚Äì Summary\n",
    "# üèÅ Conclusion\n",
    "\n",
    "1. **Model Training & Performance:**\n",
    "   - We built a **Random Forest Classifier** pipeline with:\n",
    "     - Standard Scaling\n",
    "     - SMOTE (for handling class imbalance)\n",
    "     - GridSearchCV (for hyperparameter tuning)\n",
    "   - The model achieved:\n",
    "     - **Training Accuracy:** ~0.76\n",
    "     - **Testing Accuracy:** ~0.75\n",
    "   - ‚úÖ No major overfitting ‚Üí Model generalizes well.\n",
    "\n",
    "2. **Hyperparameter Tuning:**\n",
    "   - Best parameters were selected automatically using **GridSearchCV**.\n",
    "   - This improved the model‚Äôs balance between bias and variance.\n",
    "\n",
    "3. **Evaluation Results:**\n",
    "   - **Confusion Matrix** showed most predictions are correct, with relatively few misclassifications.\n",
    "   - **ROC Curve & AUC Score** indicated the model performs significantly better than random guessing.\n",
    "   - **Classification Report** (Precision, Recall, F1-score) confirmed reliability across classes.\n",
    "\n",
    "4. **Insights from Data:**\n",
    "   - Features like **Glucose, BMI, Age, and Insulin** had strong influence on predictions (from feature importance analysis).\n",
    "   - These factors are critical indicators for disease risk.\n",
    "\n",
    "5. **Model Deployment:**\n",
    "   - Model saved as `disease_prediction_model.pkl` using **joblib**.\n",
    "   - Can be reused for predictions on new/unseen patient data.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÆ Final Thoughts:\n",
    "- The pipeline we built is **robust, interpretable, and reusable**.  \n",
    "- It can be integrated into healthcare systems for **early disease prediction**.  \n",
    "- With more data and medical expert feedback, the model can be further improved.\n",
    "\n",
    "‚úÖ Overall, this project successfully demonstrates how **Machine Learning can be applied to health data** for disease prediction.\n",
    "\n",
    "\n",
    "- The dataset was preprocessed by removing outliers, fixing skewness, and scaling features.\n",
    "- Exploratory Data Analysis (EDA) included histograms, boxplots, pairplots, and heatmaps.\n",
    "- The target variable was `Outcome`, indicating whether a person is diabetic (1) or not (0).\n",
    "- Feature selection was done using SelectKBest with `f_classif`.\n",
    "- Logistic Regression was used to train the model.\n",
    "- Hyperparameter tuning was performed using GridSearchCV.\n",
    "- The final model was saved using `joblib` and tested on new unseen data.\n",
    "- The prediction outcome was interpreted as \"Diabetic\" or \"Not Diabetic\" based on input values.\n",
    "\n",
    "‚úÖ This completes the Diabetes prediction pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95905a3-4134-47cf-8a8d-2f3ea43546b0",
   "metadata": {},
   "source": [
    "## üî∑ 2. Heart Disease Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40b476c-3a35-4bfa-ade0-ddd7ddaca786",
   "metadata": {},
   "source": [
    "‚úÖ Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92abe902-df48-4e81-a37e-98d606bf52f6",
   "metadata": {},
   "source": [
    "**Handle Missing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec93c0f-f2a6-4a96-b447-d654a13dfca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "missing_values = heart.isnull().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12c338-5094-4e0c-9bef-b2ce610fb213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîö Final Check|\n",
    "print(heart.isnull().sum())  # Now all should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0693fe8a-7686-41c9-8b09-68b4ad11e499",
   "metadata": {},
   "source": [
    "‚úÖ Check & Remove Duplicate Rows in Heart Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a8fd8-5fe8-4b0c-b0f7-44882520ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(\"Duplicate Rows:\", heart.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2bc516-66fe-4089-88b5-2d8a762d64bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058cfa14-1c04-49e7-8248-17f5cfe8fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "print(\"Remaining Duplicates:\", heart.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb0669d-7feb-4267-aa52-50377d5cb492",
   "metadata": {},
   "source": [
    "‚úÖ Step: Check Skewness in the Heart Disease Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480f0a65-70a6-4f22-861b-88c9b96821d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = heart.columns\n",
    "\n",
    "for i in columns:\n",
    "    print(f\"Skewness of {i}: {heart[i].skew()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff32d9ac-fa13-4233-9b9d-62e0694d4aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heart.skew().sort_values(ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25709938-b0da-45a0-848b-5f1427f31fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplots of numerical columns before outlier removal\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(data=heart.select_dtypes(include='number'), palette='Set2')\n",
    "plt.title(\"üì¶ Boxplot of Numerical Features (Before Outlier Removal)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbaf266-acab-40b5-9a52-3a8c3504664f",
   "metadata": {},
   "source": [
    "### üì¶ Outlier Removal Based on Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d87b1c6-44d1-421e-9127-6aacb584d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå IQR method for removing outliers from selected columns\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        df_clean = df_clean[(df_clean[col] >= lower) & (df_clean[col] <= upper)]\n",
    "    return df_clean\n",
    "\n",
    "# Apply to your heart dataset\n",
    "columns_to_clean = ['fbs', 'ca', 'oldpeak', 'chol', 'trestbps','exang']\n",
    "heart_cleaned = remove_outliers_iqr(heart, columns_to_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b967e0d-3e97-40d3-8133-85f8f04ff50e",
   "metadata": {},
   "source": [
    "### üì¶ Outlier Removal Based on Skewness\n",
    "\n",
    "We analyzed the skewness of each feature in the heart disease dataset and found several variables with high skewness values. To improve data quality and reduce model distortion, we removed outliers using the IQR method.\n",
    "\n",
    "#### üîç Features with High Skewness:\n",
    "- `fbs` (1.98)\n",
    "- `ca` (1.29)\n",
    "- `oldpeak` (1.26)\n",
    "- `chol` (1.14)\n",
    "- `trestbps` (0.71)\n",
    "- `exang` (0.73)\n",
    "\n",
    "These features were cleaned by removing extreme values that lay beyond 1.5 times the interquartile range (IQR). This step helps in normalizing the distribution and improving the performance of machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d145e49e-d0cb-438c-be48-ffa79f20b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Visualize boxplot after removing outliers\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(heart_cleaned)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplot of  Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b46311-2240-428e-8b79-9039abdf5c86",
   "metadata": {},
   "source": [
    "### üì¶ Boxplot Comparison: Before vs After Outlier Removal\n",
    "\n",
    "To improve the quality of our heart disease dataset, we applied the **IQR (Interquartile Range)** method to remove outliers from highly skewed features: `fbs`, `ca`, `oldpeak`, `chol`, and `trestbps`.\n",
    "\n",
    "The boxplots shown above provide a visual comparison:\n",
    "\n",
    "- **Before Outlier Removal:** Several columns, especially `chol`, `ca`, and `oldpeak`, displayed extreme values (outliers) that stretched the boxplot and potentially distorted the data distribution.\n",
    "- **After Outlier Removal:** The distributions have become more compact and balanced. Most extreme points have been eliminated, leading to a cleaner and more reliable dataset for training machine learning models.\n",
    "\n",
    "Outlier removal helps reduce noise, stabilize model predictions, and improve overall performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9685be6b-03ab-4816-8a7a-c94858b020aa",
   "metadata": {},
   "source": [
    "### üîç Skewness Check After Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb9855-c899-494b-9406-b10cb8983eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Skewness After Outlier Removal:\")\n",
    "heart_cleaned.skew().sort_values(ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f7c87-9244-4298-aa9e-1ff97005c24c",
   "metadata": {},
   "source": [
    "After IQR method there is still outliers in fbs, ca, oldpeak, chol, and trestbps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e4862-937b-4ec4-814f-74210591c3d4",
   "metadata": {},
   "source": [
    " Handle Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ca24b-3b59-46f6-8dd2-b7286f278c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying log transformation to skewed features\n",
    "# Applying log transformation to any feature with skewness > 1 right skewed\n",
    "# apply square root transformation when skewness is between 0.5 and 1\n",
    "new_data2=heart_cleaned.copy() #creating a copy before skewness corrections\n",
    "for col in heart_cleaned.columns:\n",
    "    if heart_cleaned[col].skew() > 1:\n",
    "        heart_cleaned[col] = np.log1p(heart_cleaned[col])\n",
    "\n",
    "print(\"\\nSkewness after log transformation:\")\n",
    "print(heart_cleaned.skew().sort_values(ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b66041f-57ab-4951-9d86-7db85c9eede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when skewness less than -0.5 go for power transformation methos like Yeo-johnson or Box-cox\n",
    "\n",
    "# Initialize PowerTransformer with Yeo-Johnson method\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "for col in heart_cleaned.columns:\n",
    "    if heart_cleaned[col].skew() < -0.5:\n",
    "        # Reshape to 2D, apply transformation, and flatten back to 1D\n",
    "        heart_cleaned[col] = pt.fit_transform(heart_cleaned[[col]]).flatten()\n",
    "\n",
    "# Print skewness in ascending order\n",
    "print(heart_cleaned.skew().sort_values(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809ad1d6-30d4-416f-a2dc-de417bc50f3a",
   "metadata": {},
   "source": [
    "### üîÉ Handling Skewness in the Heart Dataset\n",
    "\n",
    "After outlier removal, we identified some features with remaining high skewness such as `chol`, `oldpeak`, `trestbps`, `ca`,`exang`and `fbs`.\n",
    "\n",
    "To normalize these distributions, we applied the **log transformation (log1p)**. This compresses large values and spreads out small ones, reducing the skew.\n",
    "\n",
    "Handling skewness improves model performance by ensuring that features follow a more normal distribution, which benefits algorithms like Logistic Regression, SVM, and Linear models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf76d02-ddcc-44ab-86d0-a88547eab884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate categorical and numerical columns\n",
    "categorical_cols = heart_cleaned.select_dtypes(include='int64').columns.tolist()\n",
    "numerical_cols = heart_cleaned.select_dtypes(include='float64').columns.tolist()\n",
    "\n",
    "# Manually adjust if needed\n",
    "print(\"Categorical Columns:\", categorical_cols)\n",
    "print(\"Numerical Columns:\", numerical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80619f59-6d48-44e8-8cdd-da93692709cf",
   "metadata": {},
   "source": [
    "## üß™ Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ccf31b-341d-4a88-b3ac-7634e9543d7b",
   "metadata": {},
   "source": [
    "üé® 1. Histogram of age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c70934-e08a-44cb-819b-8cb374d834aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(data=heart_cleaned, x='age', bins=20, kde=True, color='skyblue')\n",
    "plt.title('Age Distribution of Patients')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0685728f-0bb0-492e-a47b-636e1350f306",
   "metadata": {},
   "source": [
    "### üßì Age Distribution\n",
    "Shows most patients are between **50‚Äì60 years**. This age range has the highest risk of heart disease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de99d33-fecb-4d12-b11a-bbe2f3df16f3",
   "metadata": {},
   "source": [
    "‚ù§Ô∏è 2. Countplot of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ad549b-506b-4235-8dfd-0356583bfe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='target', data=heart_cleaned, palette='Set2')\n",
    "plt.title('Heart Disease Presence')\n",
    "plt.xlabel('Target (0 = No, 1 = Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3380960a-18a2-4749-a9b9-ee99d27ef78f",
   "metadata": {},
   "source": [
    "### ‚ù§Ô∏è Heart Disease Count\n",
    "This shows how many patients were diagnosed with heart disease (1) and how many were not (0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc5a944-fb39-44e4-8b2d-ab85f0d88342",
   "metadata": {},
   "source": [
    "ü©∫ 3. Boxplot: chol vs target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5171a09-1717-47ee-a8f7-d048534a60cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='target', y='chol', data=heart_cleaned, palette='pastel')\n",
    "plt.title('Cholesterol vs Heart Disease')\n",
    "plt.xlabel('Heart Disease')\n",
    "plt.ylabel('Cholesterol Level')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d7fba-9ecc-44d8-a362-5b4beec4594b",
   "metadata": {},
   "source": [
    "### ü©∏ Cholesterol Levels by Heart Disease\n",
    "Patients with heart disease tend to have **higher cholesterol**. Some outliers were handled earlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42902f9f-ae36-454a-b981-b656a954fa9b",
   "metadata": {},
   "source": [
    "üî• 4. Heatmap of Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb2ba5-6193-45a7-a6ae-d116aeb07cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heart_cleaned.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fee4e28-18a3-4730-ac5f-db35c86ce4ad",
   "metadata": {},
   "source": [
    "### üî• Correlation Heatmap\n",
    "Highlights how features like `cp`, `thalach`, `oldpeak`, and `exang` relate to heart disease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851673e-be52-452e-b5ae-ac8747f68f30",
   "metadata": {},
   "source": [
    "üîÅ 5. Pairplot of Key Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91dd4c-a812-48d9-bc95-1616c89d2ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['cp', 'thalach', 'oldpeak', 'age', 'target']\n",
    "sns.pairplot(heart_cleaned[top_features], hue='target', palette='Set1')\n",
    "plt.suptitle('Pairwise Relationships', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b4f98f-3c91-447b-95ef-097c45db0a2f",
   "metadata": {},
   "source": [
    "### üîÅ Pairplot of Key Features\n",
    "Shows visual separation between heart disease and non-disease cases using combinations of top predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b8a38-6487-4488-9e91-50b2ddfdd010",
   "metadata": {},
   "source": [
    "6 üìä Pie Chart: Heart Disease vs No Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad33532-3b00-4e93-9d8d-62ffce7d1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the target values (0 = No disease, 1 = Disease)\n",
    "labels = ['No Disease', 'Heart Disease']\n",
    "sizes = heart_cleaned['target'].value_counts().values\n",
    "colors = ['#66b3ff', '#ff9999']\n",
    "\n",
    "# Plot the pie chart\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(sizes, labels=labels, colors=colors, startangle=90, \n",
    "        autopct='%1.1f%%', shadow=True, explode=(0, 0.1))\n",
    "plt.title('Heart Disease Distribution')\n",
    "plt.axis('equal')  # Ensures pie is circular\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc203ad6-9601-4369-8457-a5c6cab69e4b",
   "metadata": {},
   "source": [
    "### ü•ß Pie Chart: Heart Disease Distribution\n",
    "\n",
    "This pie chart shows the proportion of patients with and without heart disease:\n",
    "\n",
    "- **Red** (Heart Disease): Represents the percentage of patients diagnosed with heart conditions.\n",
    "- **Blue** (No Disease): Represents the percentage of healthy patients.\n",
    "\n",
    "The distribution appears fairly balanced, making it suitable for binary classification models without applying resampling techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb91c39-c702-4802-b943-2f6005f13644",
   "metadata": {},
   "source": [
    "### üß† Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772fb055-15bf-4f42-bb0e-af9a45b01a88",
   "metadata": {},
   "source": [
    "‚úÖ One-Hot Encoding for Heart Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb48bf-4154-429d-9c33-ee0b1f414450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply One-Hot Encoding to necessary categorical columns\n",
    "heart_encoded = pd.get_dummies(heart_cleaned, columns=['cp', 'restecg', 'slope', 'thal'], drop_first=True)\n",
    "\n",
    "# Check new columns\n",
    "heart_encoded.head()\n",
    "print(\"heart_encodedOne-Hot Encoded Data:\")\n",
    "print(heart_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a32c8d-0207-4665-af2c-a5a3799470bd",
   "metadata": {},
   "source": [
    "## üî° Encoding Categorical Variables (One-Hot Encoding)\n",
    "\n",
    "In our heart disease dataset, some features such as `cp`, `restecg`, `slope`, and `thal` are **categorical with multiple values**.\n",
    "\n",
    "### Why One-Hot Encoding?\n",
    "- These features are **nominal** (no natural order between values).\n",
    "- Using **Label Encoding** would assign numbers (0, 1, 2...) which adds a **false sense of order**.\n",
    "- **One-Hot Encoding** creates separate binary columns for each category, avoiding misinterpretation by machine learning algorithms.\n",
    "\n",
    "### Columns Encoded:\n",
    "- `cp`: Chest pain type\n",
    "- `restecg`: Resting ECG results\n",
    "- `slope`: Slope of the ST segment\n",
    "- `thal`: Thalassemia type\n",
    "\n",
    "We used `pd.get_dummies()` with `drop_first=True` to avoid multicollinearity.\n",
    "\n",
    "This ensures the data is now completely numeric and ready for model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19241a9-bdb9-478d-b95c-d67be3b36e4f",
   "metadata": {},
   "source": [
    "üìä Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a3def-97e5-49d3-ac2a-7562be173f4a",
   "metadata": {},
   "source": [
    "### üîç Correlation Heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f595bae9-7a79-43bf-99b0-184f3534018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for the encoded dataset\n",
    "corr_matrix = heart_encoded.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(14,10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('üîç Correlation Heatmap of Heart Disease Features', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344229f4-39bb-4284-a9ff-8d4b47fa4d01",
   "metadata": {},
   "source": [
    "### üéØ Target Column: `target`\n",
    "\n",
    "The `target` column is the label we want to predict using machine learning.\n",
    "\n",
    "- `0` = Patient **does not** have heart disease\n",
    "- `1` = Patient **has** heart disease\n",
    "\n",
    "This is a **binary classification problem**, where the model learns to classify whether a person is likely to have heart disease based on their health features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b61537b-1740-426f-89ce-5acc1fef2761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation with target variables\n",
    "print(heart_encoded.corr()['target'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f9ad69-f355-46da-951c-b6d739a97c41",
   "metadata": {},
   "source": [
    "### üìä Feature Selection using SelectKBest\n",
    "\n",
    "We used the **SelectKBest** method with `f_classif` (ANOVA F-value) to select the top 8 most relevant features for predicting heart disease.\n",
    "\n",
    "This method helps reduce model complexity, improves accuracy, and avoids overfitting.\n",
    "\n",
    "**Selected Features:**\n",
    "- Based on both correlation and F-score, features like `thal_2`, `thalach`, `slope_2`, `cp_2`, `oldpeak`, and `ca` were found to be the most important.\n",
    "\n",
    "These features were used to train the machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd260b-a46e-434e-ac89-c3ecbf50848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = heart_encoded.drop('target', axis=1)\n",
    "y = heart_encoded['target']\n",
    "\n",
    "# Apply SelectKBest to select top 8 features\n",
    "selector = SelectKBest(score_func=f_classif, k=8)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"‚úÖ Selected Features:\", selected_features.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c89aeb-ecde-4ea4-a2d8-fa644a8c3b67",
   "metadata": {},
   "source": [
    "### ‚úÖ Feature Selection (Top 8)\n",
    "\n",
    "Used `SelectKBest` with ANOVA F-test (`f_classif`) to select the top 8 most relevant features for predicting heart disease. These features will be used for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc3c4c-dc52-4834-a997-41cb535699e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_scores = selector.scores_[selector.get_support()] # to find scores of all features\n",
    "print(\"Feature Scores based on selector:\", selected_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833dbb68-a580-4bef-845c-514daf4a4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on k scores we can choose number of features required\n",
    "selector = SelectKBest(score_func=f_classif, k=2)  # Selecting top 2 features (depends on user choice)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "print(\"Selected Features:\", X.columns[selector.get_support()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67398921-7440-496f-a72c-caa9e90568a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = heart_encoded.drop('target', axis=1)\n",
    "y = heart_encoded['target']\n",
    "\n",
    "# Split the data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c3eef-c7fb-4020-a3fd-48c977a65f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3b9f60-30c5-4524-9d47-8e4d27f49d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b771ce3-d3e4-4ff9-b932-660815e98f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80aca3-88bb-4ec6-be04-6883b62a9de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1641a-4b86-4cbd-80bb-f8bb1571587e",
   "metadata": {},
   "source": [
    "Apply SMOTE to Heart Disease Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0951aefb-3094-4133-82be-427ac88d8f03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot before SMOTE\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Class Distribution Before SMOTE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab41f2-72a8-43b9-89de-5e92d6edee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying SMOTE\n",
    "print(\"Original Class Distribution:\", y_train.value_counts())\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "print(\"Resampled Class Distribution:\", pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9892bda-6fb3-4438-b7b0-dca9f445d266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ‚úÖ After SMOTE\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=y_train, palette=\"Set1\")\n",
    "plt.title(\"üìà Class Distribution After SMOTE\")\n",
    "plt.xlabel(\"target\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f200dcd-3b3d-4506-a85f-5cdf82ccc8d5",
   "metadata": {},
   "source": [
    "### SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "SMOTE is a popular technique used to **balance imbalanced datasets**.  \n",
    "It works by **generating synthetic samples** for the minority class rather than simply duplicating existing ones.  \n",
    "\n",
    "**Key Points:**\n",
    "- Helps improve model performance on imbalanced classification tasks.\n",
    "- Reduces bias toward the majority class.\n",
    "- Creates new samples by interpolating between existing minority class examples.\n",
    "\n",
    "**Usage:** Often applied **before training a model** to ensure the classifier sees a balanced dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8413e026-20bc-401c-af2d-d69c3af75bd4",
   "metadata": {},
   "source": [
    "### ‚öñÔ∏è Feature Scaling ‚Äì Heart Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff1f68b-2509-4d7a-9c3c-11951a6585fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "print(X_train_scaled )\n",
    "y_train_df = pd.DataFrame(y_train) #converting to data frame from series\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(y_train_df)\n",
    "y_train_scaled = scaler.transform(y_train_df)\n",
    "print(y_train_scaled )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a98ef-e185-4e11-84e8-c864f740a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e407b72-6943-4821-98e1-d920e8e28aa6",
   "metadata": {},
   "source": [
    "### ü§ñ Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d49a89-7d36-495c-9908-ab3360a59e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'SVM': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"\\n\")\n",
    "    # Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\n",
    "results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854088ea-acf2-491e-ba29-0680df56abb8",
   "metadata": {},
   "source": [
    "### Best Model for Heart Disease Prediction\n",
    "\n",
    "After evaluating multiple classification models on the heart disease dataset, **Gradient Boosting** emerged as the best-performing model.  \n",
    "\n",
    "**Key Metrics:**\n",
    "- **Accuracy:** 0.80  \n",
    "- **Precision:** 0.65  \n",
    "- **Recall:** 0.78  \n",
    "- **F1 Score:** 0.71  \n",
    "\n",
    "**Why Gradient Boosting is Best:**\n",
    "- Achieves the **highest accuracy** among all tested models.  \n",
    "- **High recall** ensures most patients with heart disease are correctly identified.  \n",
    "- **Balanced F1 score** indicates good trade-off between precision and recall.  \n",
    "\n",
    "> Gradient Boosting is therefore recommended for predicting heart disease in this dataset.\n",
    "> # üèÜ Model Selection\n",
    "\n",
    "- After comparing metrics:\n",
    "  - **Accuracy** shows overall performance.\n",
    "  - **Recall** is very important in disease prediction (catching positive cases).\n",
    "  - **F1-Score** balances precision & recall.\n",
    "\n",
    "üëâ The model with the **highest Recall & F1-score** is usually preferred for healthcare problems, since missing a patient with disease is riskier than false alarms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9ae13-c495-462b-b9dd-b024c2b5c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=\"Model\", y=\"Accuracy\", data=results_df)\n",
    "plt.title(\"Model Accuracy Comparison\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faec3f1-8c51-43d4-9715-943b786c9496",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, \"predict_proba\"):  # Only models with predict_proba\n",
    "        y_prob = model.predict_proba(X_test)[:,1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "plt.plot([0,1],[0,1],\"--\",color=\"gray\")\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421f676-2895-4879-85d0-e75f099cc764",
   "metadata": {},
   "source": [
    "### üîπ ROC Curve Comparison\n",
    "\n",
    "The ROC (Receiver Operating Characteristic) curve is used to evaluate the performance of classification models.  \n",
    "- The **X-axis** represents the False Positive Rate (FPR), and the **Y-axis** represents the True Positive Rate (TPR).  \n",
    "- The **diagonal dashed line** represents a random classifier (AUC = 0.5).  \n",
    "\n",
    "From the graph:  \n",
    "- **Random Forest (AUC = 0.87)** performs the best among all models.  \n",
    "- **Logistic Regression (AUC = 0.86)** also shows strong performance.  \n",
    "- **Gradient Boosting (AUC = 0.82)** performs moderately well.  \n",
    "- **Decision Tree (AUC = 0.65)** shows the weakest performance.  \n",
    "\n",
    "‚úÖ Higher AUC (Area Under the Curve) values indicate better model performance in distinguishing between positive and negative cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed2427c-32fe-4d5f-a0c7-98d46b717959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [None, 5, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 5, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "best_models = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"üîç Tuning {name}...\")\n",
    "    grid = GridSearchCV(model, param_grids[name], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best Parameters for {name}: {grid.best_params_}\")\n",
    "    print(f\"Best CV Accuracy: {grid.best_score_:.4f}\\n\")\n",
    "    \n",
    "    best_models[name] = grid.best_estimator_\n",
    "\n",
    "# Evaluate tuned models\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Test Accuracy after tuning: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669128f-99da-4626-a810-4f1a352cb47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a simple pipeline for preprocessing and model training\n",
    "pipeline = ImbPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', LogisticRegression(penalty=\"l2\", C=0.1, max_iter=1000))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c07f61-bb34-4655-9e41-7ed5b896fd30",
   "metadata": {},
   "source": [
    "\n",
    "üíæ Save the Best Heart Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf2fad-f9a5-4463-b587-4a2f53a0f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the pipeline\n",
    "joblib.dump(grid_search.best_estimator_, 'best_heart_model.pkl')\n",
    "print(\"üíæ Heart model saved successfully as 'best_heart_model.pkl'\")\n",
    "\n",
    "# Load the model later\n",
    "best_model = joblib.load('best_heart_model.pkl')\n",
    "y_pred_loaded = best_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f347ba-b2cb-4cba-b282-0e88029d3b71",
   "metadata": {},
   "source": [
    "Create Unseen Patient Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5f40d-85ee-4880-b1bf-1f0764d52d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example new patient data (same features as training set)\n",
    "new_data = pd.DataFrame({\n",
    "    'age': [52, 60],\n",
    "    'sex': [1, 0],\n",
    "    'cp': [3, 2],\n",
    "    'trestbps': [130, 140],\n",
    "    'chol': [250, 220],\n",
    "    'fbs': [0, 1],\n",
    "    'restecg': [1, 0],\n",
    "    'thalach': [160, 150],\n",
    "    'exang': [0, 1],\n",
    "    'oldpeak': [1.2, 2.5],\n",
    "    'slope': [2, 1],\n",
    "    'ca': [0, 2],\n",
    "    'thal': [2, 3]\n",
    "})\n",
    "\n",
    "# Use loaded pipeline for prediction\n",
    "predictions = loaded_pipeline.predict(new_data)\n",
    "print(\"üîç Predictions for New Data:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c52f2-e7e3-4c57-9bc8-ec3b3932d694",
   "metadata": {},
   "source": [
    "# üß™ Testing the Pipeline on New Data\n",
    "\n",
    "We provided **new patient health records** to the trained pipeline.  \n",
    "The pipeline automatically handled:\n",
    "- Data preprocessing (scaling, encoding, etc.)\n",
    "- Feeding data into the tuned Logistic Regression model  \n",
    "\n",
    "The predictions indicate **whether the patient is at risk of heart disease (1) or not (0)**.  \n",
    "This step confirms the pipeline is **ready for real-world use**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a3184-97df-424c-be35-ef312f212e65",
   "metadata": {},
   "source": [
    "# üèÅ Final Conclusion\n",
    "\n",
    "1. **Models Trained:** Logistic Regression, SVM, Decision Tree, Random Forest, Gradient Boosting  \n",
    "2. **Best Performing Model:** Logistic Regression (after Hyperparameter Tuning) with ~86.8% CV Accuracy  \n",
    "3. **Data Balancing:** Applied SMOTE to handle imbalance in Heart Disease dataset, which improved recall (better at identifying patients with disease).  \n",
    "4. **Pipeline:** Created and saved a reusable pipeline (`disease_prediction_pipeline.pkl`) that handles preprocessing and prediction automatically.  \n",
    "5. **New Data Testing:** The pipeline successfully predicted outcomes on unseen patient records, confirming generalization.  \n",
    "\n",
    "---\n",
    "\n",
    "### üîë Key Insights:\n",
    "- Logistic Regression works very well for structured medical data with limited features.  \n",
    "- Random Forest and Gradient Boosting also performed strongly but required more tuning.  \n",
    "- Using SMOTE helped prevent bias toward the majority class.  \n",
    "- The project demonstrates how **Machine Learning can assist doctors** by predicting health risks (Diabetes, Heart Disease, Parkinson‚Äôs).  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Next Work:\n",
    "- Deploy the model into a simple web or mobile app for real-time predictions.  \n",
    "- Add more datasets or features (e.g., lifestyle, genetic data) to improve accuracy.  \n",
    "- Apply Explainable AI (like SHAP or LIME) to understand feature importance in predictions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5e184-ad62-477c-9650-9636c2cfa7de",
   "metadata": {},
   "source": [
    "## üî∑ 2. Parkinson‚Äôs Disease Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0721066-f4ad-464a-9a27-0095af787a27",
   "metadata": {},
   "source": [
    "‚úÖ Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c50dfe-1dd5-4681-b035-5f3ea2293a61",
   "metadata": {},
   "source": [
    "**Handle Missing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014daad0-1836-43f3-9a92-5ecb0e307096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "missing_values = parkinsons.isnull().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e8505-4536-4b38-bacc-47edb452deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîö Final Check|\n",
    "print(parkinsons.isnull().sum())  # Now all should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35743c09-bb95-402f-9328-d19e1086cdea",
   "metadata": {},
   "source": [
    "‚úÖ Check & Remove Duplicate Rows in Heart Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e950adbf-14ad-483b-a685-a856e23b47a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "print(\"Duplicate Rows:\", parkinsons.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee049fa8-2239-4257-824e-7e89e49ab380",
   "metadata": {},
   "source": [
    "step: Check Skewness in the parkinsons Disease Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf4afe9-cb69-4857-8964-fac93aec3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only process numeric columns\n",
    "numeric_columns = parkinsons.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "for col in numeric_columns:\n",
    "    print(f\"Skewness of {col}: {parkinsons[col].skew():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba613060-9011-47de-845e-e74597baa369",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop non-numeric column like 'name' if present\n",
    "parkinsons_numeric = parkinsons.select_dtypes(include='number')\n",
    "\n",
    "# üì¶ Boxplot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(data=parkinsons_numeric, palette='Set2')\n",
    "plt.title(\"üì¶ Boxplot of Numerical Features (Before Outlier Removal)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e870b9c8-92e4-4ac7-9946-53e6b9265ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR and filter out outliers\n",
    "Q1 = parkinsons_numeric.quantile(0.25)\n",
    "Q3 = parkinsons_numeric.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Keep only rows within the IQR range\n",
    "parkinsons_cleaned = parkinsons_numeric[~((parkinsons_numeric < (Q1 - 1.5 * IQR)) | (parkinsons_numeric > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "print(f\"‚úÖ Rows before: {parkinsons_numeric.shape[0]}, after removing outliers: {parkinsons_cleaned.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5ad41-bf66-4b13-b967-2d1088f4989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot after outlier removal\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(data=parkinsons_cleaned, palette='Set3')\n",
    "plt.title(\"üì¶ Boxplot of Numerical Features (After Outlier Removal)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1899a8-8a2c-4608-ab3c-d9332d22f929",
   "metadata": {},
   "source": [
    "### üîç Skewness Check After Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb2511-177e-459d-bfce-4955ece5f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Skewness After Outlier Removal:\")\n",
    "parkinsons_cleaned.skew().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b854d0-8721-47af-a603-7b758c5d4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying log transformation to skewed features\n",
    "# Applying log transformation to any feature with skewness > 1 right skewed\n",
    "# apply square root transformation when skewness is between 0.5 and 1\n",
    "new_data2=parkinsons_cleaned.copy() #creating a copy before skewness corrections\n",
    "for col in parkinsons_cleaned.columns:\n",
    "    if parkinsons_cleaned[col].skew() > 1:\n",
    "        parkinsons_cleaned[col] = np.log1p(parkinsons_cleaned[col])\n",
    "\n",
    "print(\"\\nSkewness after log transformation:\")\n",
    "print(parkinsons_cleaned.skew().sort_values(ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4adf116-9043-450a-8171-a18c4b1deaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when skewness less than -0.5 go for power transformation methos like Yeo-johnson or Box-cox\n",
    "\n",
    "# Initialize PowerTransformer with Yeo-Johnson method\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "for col in parkinsons_cleaned.columns:\n",
    "    if parkinsons_cleaned[col].skew() < -0.5:\n",
    "        # Reshape to 2D, apply transformation, and flatten back to 1D\n",
    "        parkinsons_cleaned[col] = pt.fit_transform(parkinsons_cleaned[[col]]).flatten()\n",
    "\n",
    "# Print skewness in ascending order\n",
    "print(parkinsons_cleaned.skew().sort_values(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc59db-4642-4782-bcca-ffcec408675c",
   "metadata": {},
   "source": [
    "### üìä Interpretation of Boxplot After Outlier Removal\n",
    "\n",
    "The boxplot above shows the distribution of the numerical features in the Parkinson‚Äôs dataset **after removing outliers** using the IQR (Interquartile Range) method.\n",
    "\n",
    "#### ‚úÖ What to observe:\n",
    "- The whiskers of each box now better represent the true spread of the data.\n",
    "- Extreme values (previously shown as individual points) are mostly removed, reducing noise.\n",
    "- The central boxes (representing the interquartile range) are now tighter and more consistent, which helps in building better models.\n",
    "\n",
    "#### üß† Why this is useful:\n",
    "Outliers can negatively impact machine learning models, especially those sensitive to scale (like Logistic Regression, SVM, and KNN). By removing them:\n",
    "- We make training more stable.\n",
    "- We improve model performance and generalization.\n",
    "- We avoid misleading results due to extreme values.\n",
    "\n",
    "Next, we‚Äôll proceed to **scaling** the cleaned dataset before applying machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d6a98-b8a6-407e-9d0b-b1352f51dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîö Final Check\n",
    "print(parkinsons.isnull().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd64528-4068-4cfe-9cb0-173570ced0cd",
   "metadata": {},
   "source": [
    "‚úÖ Check & Remove Duplicate Rows in Heart Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674c578-390f-427e-b181-090f76b2f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "print(\"Duplicate Rows:\", parkinsons.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a98a0-7121-4141-bd05-ad668d2b5474",
   "metadata": {},
   "source": [
    "Step: Check Skewness in the Heart Disease Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d4da3-fa5a-4641-b02f-bd392d86325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only process numeric columns\n",
    "numeric_columns = parkinsons.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "for col in numeric_columns:\n",
    "    print(f\"Skewness of {col}: {parkinsons[col].skew():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72c836-f7d0-4895-ab50-128cddc4379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop non-numeric column like 'name' if present\n",
    "parkinsons_numeric = parkinsons.select_dtypes(include='number')\n",
    "\n",
    "# üì¶ Boxplot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(data=parkinsons_numeric, palette='Set2')\n",
    "plt.title(\"üì¶ Boxplot of Numerical Features (Before Outlier Removal)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd30859-9733-4012-a3f8-a5a2946c5f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR and filter out outliers\n",
    "Q1 = parkinsons_numeric.quantile(0.25)\n",
    "Q3 = parkinsons_numeric.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Keep only rows within the IQR range\n",
    "parkinsons_cleaned = parkinsons_numeric[~((parkinsons_numeric < (Q1 - 1.5 * IQR)) | (parkinsons_numeric > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "print(f\"‚úÖ Rows before: {parkinsons_numeric.shape[0]}, after removing outliers: {parkinsons_cleaned.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc44172d-8b17-4902-8b50-65d79089c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot after outlier removal\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(data=parkinsons_cleaned, palette='Set3')\n",
    "plt.title(\"üì¶ Boxplot of Numerical Features (After Outlier Removal)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501628b-e867-4937-ae67-05b60c0b192f",
   "metadata": {},
   "source": [
    "### üîç Skewness Check After Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d7c1f0-4271-4fd3-8f24-36c2260fda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Skewness After Outlier Removal:\")\n",
    "parkinsons_cleaned.skew().sort_values(ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4f4405-0043-4a22-9501-944d912af56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying log transformation to skewed features\n",
    "# Applying log transformation to any feature with skewness > 1 right skewed\n",
    "# apply square root transformation when skewness is between 0.5 and 1\n",
    "new_data2=parkinsons_cleaned.copy() #creating a copy before skewness corrections\n",
    "for col in parkinsons_cleaned.columns:\n",
    "    if parkinsons_cleaned[col].skew() > 1:\n",
    "        parkinsons_cleaned[col] = np.log1p(parkinsons_cleaned[col])\n",
    "\n",
    "print(\"\\nSkewness after log transformation:\")\n",
    "print(parkinsons_cleaned.skew().sort_values(ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbbbc7-ea25-4649-be32-0a6b3be6b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when skewness less than -0.5 go for power transformation methos like Yeo-johnson or Box-cox\n",
    "\n",
    "# Initialize PowerTransformer with Yeo-Johnson method\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "for col in parkinsons_cleaned.columns:\n",
    "    if parkinsons_cleaned[col].skew() < -0.5:\n",
    "        # Reshape to 2D, apply transformation, and flatten back to 1D\n",
    "        parkinsons_cleaned[col] = pt.fit_transform(parkinsons_cleaned[[col]]).flatten()\n",
    "\n",
    "# Print skewness in ascending order\n",
    "print(parkinsons_cleaned.skew().sort_values(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f71fb-ff7d-470b-a335-a4cd7a0df2a1",
   "metadata": {},
   "source": [
    "### üìä Interpretation of Boxplot After Outlier Removal\n",
    "\n",
    "The boxplot above shows the distribution of the numerical features in the Parkinson‚Äôs dataset **after removing outliers** using the IQR (Interquartile Range) method.\n",
    "\n",
    "#### ‚úÖ What to observe:\n",
    "- The whiskers of each box now better represent the true spread of the data.\n",
    "- Extreme values (previously shown as individual points) are mostly removed, reducing noise.\n",
    "- The central boxes (representing the interquartile range) are now tighter and more consistent, which helps in building better models.\n",
    "\n",
    "#### üß† Why this is useful:\n",
    "Outliers can negatively impact machine learning models, especially those sensitive to scale (like Logistic Regression, SVM, and KNN). By removing them:\n",
    "- We make training more stable.\n",
    "- We improve model performance and generalization.\n",
    "- We avoid misleading results due to extreme values.\n",
    "\n",
    "Next, we‚Äôll proceed to **scaling** the cleaned dataset before applying machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abde539e-4fee-4847-882d-c8283652e851",
   "metadata": {},
   "source": [
    "## üß™ Exploratory Data Analysis (EDA)\n",
    "This section explores how each health-related feature in the dataset behaves individually, in pairs, and across multiple variables. These visuals help us understand patterns and relationships that are critical for building accurate AI disease prediction models.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4894fd32-b7de-42ad-8fa9-fda0209c32d8",
   "metadata": {},
   "source": [
    "üìä Count Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b44349-1b61-412e-8a18-f1c336b5aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Draw the count plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=parkinsons, x='status', palette='Set2')\n",
    "plt.title('üß† Parkinson\\'s Disease Count by Status')\n",
    "plt.xlabel('Status (0 = Healthy, 1 = Parkinson\\'s)')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84ed60-ad46-4b87-936a-85a3672b548c",
   "metadata": {},
   "source": [
    "### üìä Count Plot ‚Äì Parkinson's Disease Status\n",
    "\n",
    "This count plot displays the number of patients in the dataset who are:\n",
    "\n",
    "- `0`: Healthy individuals (no Parkinson's)\n",
    "- `1`: Patients diagnosed with Parkinson's disease\n",
    "\n",
    "üß† **Purpose**:\n",
    "- To understand class distribution.\n",
    "- It reveals if the dataset is **balanced** or **imbalanced**, which is crucial for model training.\n",
    "\n",
    "üìå **Observation**:\n",
    "- If one class dominates, model performance might be biased.\n",
    "- A balanced dataset generally leads to better classification outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c25c12-fa8c-4a27-b65b-e62be9097160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "corr_matrix = parkinsons_cleaned.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"üîó Correlation Heatmap of Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a6556-f211-483e-8faa-0f7c4f5e62ae",
   "metadata": {},
   "source": [
    "### üîó Correlation Heatmap\n",
    "\n",
    "This heatmap shows the correlation between all numerical features in the Parkinson‚Äôs dataset.\n",
    "\n",
    "- Values range from **-1 to +1**\n",
    "- **+1** indicates a strong positive correlation\n",
    "- **-1** indicates a strong negative correlation\n",
    "- **0** means no correlation\n",
    "\n",
    "We use this to:\n",
    "- Detect **multicollinearity** (features that are strongly correlated with each other)\n",
    "- Identify features strongly related to the target (`status`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0292d-f190-4dd6-a23f-3ed33a959495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßÆ 2. Histogram / KDE Plot\n",
    "# Define the features (exclude 'status' and non-numeric if any)\n",
    "features = [col for col in parkinsons_cleaned.columns if col != 'status']\n",
    "\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(data=parkinsons_cleaned, x=feature, hue='status', kde=True, palette='viridis', bins=30)\n",
    "    plt.title(f\"üßÆ Distribution of {feature} by Status\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf561c28-491a-45ba-83a3-114e726dbbea",
   "metadata": {},
   "source": [
    "### üßÆ Histogram & KDE Plots\n",
    "\n",
    "These plots show the distribution of each feature, grouped by `status`.\n",
    "\n",
    "- Helps us understand how values are spread between healthy and Parkinson‚Äôs patients\n",
    "- KDE line shows a smooth approximation of the distribution curve\n",
    "- Good for spotting skewness or feature separation between classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d142d5-0234-4ff0-b6b1-c856bc9291cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßë‚Äçü§ù‚Äçüßë 3. Pair Plot\n",
    "# Only select a few important features for readability\n",
    "subset_features = ['status', 'MDVP:Fo(Hz)', 'MDVP:Jitter(%)', 'MDVP:Shimmer', 'spread1', 'D2']\n",
    "\n",
    "sns.pairplot(parkinsons_cleaned[subset_features], hue='status', palette='coolwarm')\n",
    "plt.suptitle(\"üßë‚Äçü§ù‚Äçüßë Pair Plot of Selected Features\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75650e7-d1e9-4b40-821f-d5e02273fc40",
   "metadata": {},
   "source": [
    "### üßë‚Äçü§ù‚Äçüßë Pair Plot\n",
    "\n",
    "The pair plot shows the relationships between selected features and how they cluster by `status`.\n",
    "\n",
    "- Each cell represents a scatterplot between two features.\n",
    "- Diagonal shows the KDE or histogram for each individual feature.\n",
    "- Helps identify **separable groups** and **feature interactions** useful for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64872180-5a70-4c4e-939a-4893c90d1078",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 6. Violin Plot\n",
    "# A violin plot combines boxplot and KDE. It helps visualize the distribution + spread of features by class (status).\n",
    "# Choose a few key features for visual clarity\n",
    "violin_features = ['MDVP:Fo(Hz)', 'MDVP:Jitter(%)', 'MDVP:Shimmer', 'spread1']\n",
    "\n",
    "for feature in violin_features:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.violinplot(x='status', y=feature, data=parkinsons_cleaned, palette='Set2')\n",
    "    plt.title(f\"üéª Violin Plot of {feature} by Status\")\n",
    "    plt.xlabel(\"Status (0 = Healthy, 1 = Parkinson's)\")\n",
    "    plt.ylabel(feature)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b94e12-7841-4800-b7d3-9ae132eeb0cf",
   "metadata": {},
   "source": [
    "### üéª Violin Plot\n",
    "\n",
    "The violin plot combines boxplot and distribution (KDE) to show how feature values are spread across each class.\n",
    "\n",
    "- Wider sections show higher density of data points.\n",
    "- Narrow sections mean fewer data points.\n",
    "- Helps to spot if a feature has a **distinctive distribution** between Parkinson‚Äôs and healthy groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a9523b-566f-49d2-b9cf-c7bd502e6a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = parkinsons_cleaned.select_dtypes(include='object').columns\n",
    "print(\"Categorical columns:\", categorical_cols.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7722d17-303c-4803-8cb3-bb7c2ead3212",
   "metadata": {},
   "source": [
    "### üß¨ Feature Encoding (Not Required)\n",
    "\n",
    "In this dataset, all features are already **numerical**, so no encoding is needed.\n",
    "\n",
    "- Feature encoding is typically used to convert **categorical variables** (like gender, region, or type) into a numerical format.\n",
    "- Since the Parkinson‚Äôs dataset contains **no object or categorical columns**, we can directly proceed to the next steps like **scaling**, **feature selection**, and **model training**.\n",
    "\n",
    "‚úÖ This saves preprocessing time and simplifies the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec53510-55d1-461a-96f9-3ed5bd44980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = parkinsons_cleaned.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True, linewidths=0.5)\n",
    "plt.title(\"üîó Correlation Heatmap of Parkinson's Features\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870b6fa-6937-4768-bf0f-7e4af4ab5ad8",
   "metadata": {},
   "source": [
    "### üéØ Target Variable ‚Äì `status`\n",
    "\n",
    "In the Parkinson‚Äôs dataset, the **target column** is `status`.\n",
    "\n",
    "- It indicates whether a person has **Parkinson‚Äôs disease** or not.\n",
    "- It is a **binary classification** variable:\n",
    "  - `0` = Healthy (No Parkinson‚Äôs)\n",
    "  - `1` = Parkinson‚Äôs Disease (Positive)\n",
    "\n",
    "#### Why it's important:\n",
    "- All other columns are considered **input features** used to predict this target.\n",
    "- During model training, the algorithm learns to distinguish between `status = 0` and `status = 1` based on the values of the other features.\n",
    "\n",
    "We use this target for classification tasks to build a model that can predict whether a new patient is likely to h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c0b6b-845c-4a0e-8415-9ae0783dd514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index before joining\n",
    "parkinsons_cleaned = parkinsons_cleaned.reset_index(drop=True)\n",
    "parkinsons = parkinsons.reset_index(drop=True)\n",
    "\n",
    "# Now assign the 'status' column directly\n",
    "parkinsons_cleaned['status'] = parkinsons['status']\n",
    "\n",
    "# Ensure numeric and drop any NaNs\n",
    "parkinsons_cleaned['status'] = pd.to_numeric(parkinsons_cleaned['status'], errors='coerce')\n",
    "parkinsons_cleaned = parkinsons_cleaned.dropna()\n",
    "\n",
    "# Now check correlation\n",
    "print(parkinsons_cleaned[['MDVP:Fo(Hz)', 'status']].corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12359129-8ef9-4e84-845a-237da1752775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check correlation again\n",
    "print(parkinsons_cleaned.corr()['status'].sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e1035-d957-406c-98ee-334df95fdf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = parkinsons_cleaned.drop(columns='status')\n",
    "y = parkinsons_cleaned['status']\n",
    "\n",
    "# Select top k features (e.g., top 10)\n",
    "k = 10\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_kbest = selector.fit_transform(X, y)\n",
    "\n",
    "# Get feature names\n",
    "selected_mask = selector.get_support()\n",
    "selected_features = X.columns[selected_mask]\n",
    "\n",
    "print(f\"‚úÖ Top {k} Selected Features:\")\n",
    "print(selected_features.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8b434-a2e0-48a0-ab22-452d9a150c86",
   "metadata": {},
   "source": [
    "### üîç Feature Selection using SelectKBest (ANOVA F-test)\n",
    "\n",
    "We applied `SelectKBest` with the ANOVA F-test (`f_classif`) to identify the **top 10 features** that are most statistically related to the target variable `status`.\n",
    "\n",
    "#### Why use it:\n",
    "- Automatically selects the most relevant features\n",
    "- Improves model accuracy and training efficiency\n",
    "- Filters out noisy or less informative features\n",
    "\n",
    "These selected features will now be used to build a more focused and efficient machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71640eea-94e3-411a-bdb9-3acaf11b1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Use only the selected features\n",
    "X_selected = X[selected_features]  # X is from the previous step\n",
    "y = parkinsons_cleaned['status']\n",
    "\n",
    "# Split into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Check the shape of the splits\n",
    "print(\"üìä Training Features Shape:\", X_train.shape)\n",
    "print(\"üìä Testing Features Shape:\", X_test.shape)\n",
    "print(\"üéØ Training Labels Shape:\", y_train.shape)\n",
    "print(\"üéØ Testing Labels Shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c677f1-3b04-45f5-bc1c-a310740af675",
   "metadata": {},
   "source": [
    "### ‚úÇÔ∏è Split Data into Training and Testing Sets\n",
    "\n",
    "We split the dataset into:\n",
    "- **80% training data** to train the machine learning model\n",
    "- **20% testing data** to evaluate the model‚Äôs performance on unseen data\n",
    "\n",
    "We used `train_test_split()` from scikit-learn with:\n",
    "- `random_state=42` to ensure reproducibility\n",
    "- `stratify=y` to preserve the distribution of target classes (0 and 1) in both sets\n",
    "\n",
    "This split allows us to build a model and assess its ability to generalize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fe39a7-1854-4e0c-b1db-92419f2c082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d52af1b-7d4a-4a69-82ec-55a0a801346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a497f2-792d-4d59-8e86-c36dcfe443eb",
   "metadata": {},
   "outputs": [],
   "source": [
    " y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc5fa3-c698-40af-922f-436a5d0a268e",
   "metadata": {},
   "outputs": [],
   "source": [
    " y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df623530-ba78-49b7-a9b2-f0d6060b159e",
   "metadata": {},
   "source": [
    "smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f44af-733e-424d-9381-600e3aba51e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count plot before SMOTE\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Before SMOTE - Class Distribution\")\n",
    "plt.show()\n",
    "print(\"Before SMOTE:\", Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8cebe9-e769-465e-91a2-e0d58d4476b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY IF THERE IS DATA IMBALANCE\n",
    "print(\"Original Class Distribution:\", y_train.value_counts())\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "print(\"Resampled Class Distribution:\", pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404140f8-dbfc-4e25-a85e-916a9b4dd3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Count plot after SMOTE\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=y_res)\n",
    "plt.title(\"After SMOTE - Class Distribution\")\n",
    "plt.show()\n",
    "\n",
    "print(\"After SMOTE:\", Counter(y_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65826eb-48dc-4156-9277-7a8d69171a54",
   "metadata": {},
   "source": [
    "### ‚öñÔ∏è Feature Scaling using StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e435242-85f9-4181-afc5-4a8241036de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " # Scaling using StandardScaler\n",
    "#‚úÖ Only scale feature data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "print(X_train_scaled )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be280d-cf4c-44c0-b6f1-4ebc989ff98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df = pd.DataFrame(y_train) #converting to data frame from series\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(y_train_df)\n",
    "y_train_scaled = scaler.transform(y_train_df)\n",
    "print(y_train_scaled )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae58e8b-a2b0-4655-9f81-62455857b4c4",
   "metadata": {},
   "source": [
    "### ‚öñÔ∏è Feature Scaling using StandardScaler\n",
    "\n",
    "We scaled the selected features using **StandardScaler**, which transforms each feature to have:\n",
    "- Mean = 0\n",
    "- Standard Deviation = 1\n",
    "\n",
    "This step is important for machine learning models that are sensitive to feature scale.\n",
    "\n",
    "#### ‚úÖ Notes:\n",
    "- We fit the scaler only on the **training data** to avoid data leakage.\n",
    "- The same transformation is applied to the test set using `.transform()`.\n",
    "\n",
    "Now that our data is scaled, we're ready to train machine learning models!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89773085-daeb-4d8c-a48a-c7cba6d117d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d044678-0c8c-4968-a0fc-009159655252",
   "metadata": {},
   "source": [
    "### ü§ñ Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a288ca-98ad-4ddd-8782-d8cb37d97ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000),\n",
    "    \"SVM (RBF)\": SVC(probability=True, kernel=\"rbf\", C=1.0, gamma=\"scale\", random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=None, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=300, random_state=42),\n",
    "    \"KNN (k=7)\": KNeighborsClassifier(n_neighbors=7),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=200, random_state=42),\n",
    "    \"Gaussian NB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "# ===== 3) Train with Pipeline: Scaling + SMOTE (train only) + Model =====\n",
    "results = []\n",
    "probas_for_roc = {}   # store probabilities for ROC curves\n",
    "best_models = {}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    pipe = ImbPipeline(steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"model\", clf)\n",
    "    ])\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    # Try to get probabilities for ROC-AUC (some models don‚Äôt support predict_proba)\n",
    "    try:\n",
    "        y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    except Exception:\n",
    "        try:\n",
    "            # fallback to decision_function if available\n",
    "            scores = pipe.decision_function(X_test)\n",
    "            # convert to [0,1] via min-max for AUC comparability\n",
    "            y_proba = (scores - scores.min()) / (scores.max() - scores.min() + 1e-12)\n",
    "        except Exception:\n",
    "            y_proba = None\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc_val = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan\n",
    "    \n",
    "    results.append([name, acc, prec, rec, f1, auc_val])\n",
    "    if y_proba is not None:\n",
    "        probas_for_roc[name] = y_proba\n",
    "    best_models[name] = pipe\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Accuracy :\", round(acc, 4))\n",
    "    print(\"Precision:\", round(prec, 4))\n",
    "    print(\"Recall   :\", round(rec, 4))\n",
    "    print(\"F1-score :\", round(f1, 4))\n",
    "    if y_proba is not None:\n",
    "        print(\"ROC-AUC  :\", round(auc_val, 4))\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ROC_AUC\"])\n",
    "results_df = results_df.sort_values(by=[\"F1\",\"Accuracy\"], ascending=False).reset_index(drop=True)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31882cba-e820-4657-a339-3d0022570c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot: Accuracy & F1\n",
    "plt.figure(figsize=(9,4))\n",
    "sns.barplot(x=\"Model\", y=\"Accuracy\", data=results_df)\n",
    "plt.title(\"Accuracy by Model\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "sns.barplot(x=\"Model\", y=\"F1\", data=results_df)\n",
    "plt.title(\"F1-score by Model\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curves for top models with probabilities\n",
    "plt.figure(figsize=(8,6))\n",
    "for name, y_proba in probas_for_roc.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0,1],[0,1],'k--', lw=1)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix for the best model (top row in results_df)\n",
    "best_name = results_df.iloc[0][\"Model\"]\n",
    "best_pipe = best_models[best_name]\n",
    "best_pred = best_pipe.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, best_pred)\n",
    "plt.figure(figsize=(4.8,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"viridis\", cbar=True)\n",
    "plt.title(f\"Confusion Matrix ‚Äì {best_name}\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca045d8d-5d2d-479a-87d1-d8f7e0754a17",
   "metadata": {},
   "source": [
    "üìä Model Performance Evaluation\n",
    "\n",
    "The results of multiple classification models on the Parkinson‚Äôs dataset are shown in the plots above.\n",
    "\n",
    "‚úÖ Accuracy & F1-score Comparison\n",
    "\n",
    "Most models, including Logistic Regression, Random Forest, Gaussian NB, and SVM (RBF), achieved accuracies close to 80%.\n",
    "\n",
    "The F1-score, which balances precision and recall, shows a similar trend, meaning the models are not just accurate but also reliable in handling imbalanced cases.\n",
    "\n",
    "Decision Tree performed relatively weaker compared to ensemble methods (Random Forest, Gradient Boosting, AdaBoost).\n",
    "\n",
    "üìà ROC Curve & AUC Score\n",
    "\n",
    "The ROC curves compare the ability of models to distinguish between Parkinson‚Äôs and non-Parkinson‚Äôs cases.\n",
    "\n",
    "The AUC scores show that:\n",
    "\n",
    "Random Forest, SVM (RBF), and Logistic Regression performed the best with AUC ‚âà 0.88‚Äì0.89.\n",
    "\n",
    "Gaussian NB slightly outperformed with AUC = 0.90, making it one of the strongest classifiers.\n",
    "\n",
    "Decision Tree lagged with AUC = 0.68, confirming its weaker generalization ability.\n",
    "\n",
    "üîç Confusion Matrix (Example: Gaussian NB)\n",
    "\n",
    "The confusion matrix for Gaussian Naive Bayes shows:\n",
    "\n",
    "True Positives (21) and True Negatives (16) were well captured.\n",
    "\n",
    "Only a small number of False Negatives (6) and False Positives (3) occurred.\n",
    "\n",
    "This indicates a good balance between sensitivity (recall) and specificity.\n",
    "\n",
    "üèÜ Conclusion\n",
    "\n",
    "Best Models: Gaussian NB, Random Forest, SVM (RBF), and Logistic Regression.\n",
    "\n",
    "These models achieved the highest accuracy, F1-score, and AUC, making them strong candidates for early Parkinson‚Äôs disease prediction.\n",
    "\n",
    "Decision Tree is less reliable and should be avoided for final deployment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57fcee5-6dd4-42e2-a4f3-022fe991e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# SVM\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Decision Tree\n",
    "param_grid_dt = {\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# KNN\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_rf = GridSearchCV(estimator=rf, \n",
    "                       param_grid=param_grid_rf, \n",
    "                       cv=5, \n",
    "                       scoring='accuracy',\n",
    "                       n_jobs=-1)\n",
    "\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_rf.best_params_)\n",
    "print(\"Best Accuracy:\", grid_rf.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dde0eb-94e2-4750-8d57-b94f0fcd3439",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = grid_rf.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac66862-3a05-4e58-a922-e654c52b01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define pipeline\n",
    "rf_pipeline = ImbPipeline(steps=[\n",
    "    ('scaler', StandardScaler()),       # scaling\n",
    "    ('smote', SMOTE(random_state=42)),  # handle imbalance\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [None, 5, 10],\n",
    "    'classifier__min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# GridSearchCV with pipeline\n",
    "grid_rf = GridSearchCV(rf_pipeline, param_grid=param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best RF Parameters:\", grid_rf.best_params_)\n",
    "print(\"Best RF CV Accuracy:\", grid_rf.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e1f592-820d-426a-a969-cd2197ea89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_rf.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61433372-3335-4549-9fac-aeb359bbdf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "joblib.dump(model, 'parkinsons_model.pkl')\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'parkinsons_scaler.pkl')\n",
    "\n",
    "# Save selected features list\n",
    "joblib.dump(X_train.columns.tolist(), 'parkinsons_model_columns.pkl')\n",
    "\n",
    "print(\"‚úÖ Model, scaler, and columns saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c3423-9175-4965-9973-9d194a360add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load Parkinson's model, scaler, and feature list\n",
    "model = joblib.load(\"parkinsons_model.pkl\")\n",
    "scaler = joblib.load(\"parkinsons_scaler.pkl\")\n",
    "model_columns = joblib.load(\"parkinsons_model_columns.pkl\")\n",
    "\n",
    "# New sample must match Parkinson's dataset features\n",
    "new_data = pd.DataFrame([{\n",
    "    'MDVP:Fo(Hz)': 119.992,\n",
    "    'MDVP:Fhi(Hz)': 157.302,\n",
    "    'MDVP:Flo(Hz)': 74.997,\n",
    "    'MDVP:Jitter(%)': 0.00784,\n",
    "    'MDVP:Jitter(Abs)': 0.00007,\n",
    "    'MDVP:RAP': 0.0037,\n",
    "    'MDVP:PPQ': 0.00554,\n",
    "    'Jitter:DDP': 0.01109,\n",
    "    'MDVP:Shimmer': 0.04374,\n",
    "    'MDVP:Shimmer(dB)': 0.426,\n",
    "    'Shimmer:APQ3': 0.02182,\n",
    "    'Shimmer:APQ5': 0.0313,\n",
    "    'MDVP:APQ': 0.02971,\n",
    "    'Shimmer:DDA': 0.06545,\n",
    "    'NHR': 0.02211,\n",
    "    'HNR': 21.033,\n",
    "    'RPDE': 0.414783,\n",
    "    'DFA': 0.815285,\n",
    "    'spread1': -4.813031,\n",
    "    'spread2': 0.266482,\n",
    "    'D2': 2.301442,\n",
    "    'PPE': 0.284654\n",
    "}])\n",
    "\n",
    "# Reorder and fill any missing columns\n",
    "new_data = new_data.reindex(columns=model_columns, fill_value=0)\n",
    "\n",
    "# Scale and predict\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "prediction = model.predict(new_data_scaled)\n",
    "\n",
    "print(\"‚úÖ Prediction:\", \"Parkinson‚Äôs Detected\" if prediction[0] == 1 else \"Healthy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cefdb98-f86b-4c12-a4ef-59bf3a9286df",
   "metadata": {},
   "source": [
    "üìù Conclusion\n",
    "\n",
    "In this project, we successfully developed an AI-powered disease prediction system using machine learning. By training and evaluating models on three different datasets‚ÄîDiabetes, Heart Disease, and Parkinson‚Äôs Disease‚Äîwe demonstrated how health data can be leveraged to assist in early disease detection.\n",
    "\n",
    "Each disease model was trained separately with proper scaling, preprocessing, and feature selection.\n",
    "\n",
    "The trained models were saved and integrated into a single system, where users can choose the disease type and provide their health parameters to get predictions.\n",
    "\n",
    "This approach reduces the risk of feature mismatch errors by keeping models independent, while still offering a unified platform for multiple diseases.\n",
    "\n",
    "Although the models provide useful insights, they are not a substitute for professional medical advice. The system can be seen as a support tool for early screening, helping patients and doctors to make more informed decisions.\n",
    "\n",
    "With further improvements such as larger datasets, hyperparameter tuning, and deployment in a user-friendly web app (e.g., Streamlit), this project can evolve into a practical healthcare application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cee05d-9f86-4ee4-b09e-4fe7e85cd2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
